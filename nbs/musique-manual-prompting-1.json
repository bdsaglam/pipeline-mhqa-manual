[
    {
        "rev": "workspace",
        "name": null,
        "data": {
            "rev": "workspace",
            "timestamp": null,
            "params": {
                "pipelines/research-mhqa-evaluation/params.yaml": {
                    "data": {
                        "dataset": {
                            "path": "bdsaglam/musique",
                            "name": "answerable",
                            "split": "validation"
                        },
                        "qa": {
                            "model": "llama-3-70b-tgi",
                            "temperature": 0.1,
                            "system_prompt": "no-role.txt",
                            "user_prompt_template": "cq.txt",
                            "few_shot_examples": "empty.json"
                        },
                        "run": 1
                    }
                },
                "pipelines/qa-prompt-optim/params.yaml": {
                    "data": {
                        "train": {
                            "dataset": {
                                "path": "bdsaglam/musique-mini",
                                "name": "answerable",
                                "split": "train"
                            },
                            "optimizer": "noop"
                        },
                        "qa": {
                            "model": "llama-3-70b-tgi",
                            "temperature": 0.1,
                            "technique": "standard"
                        },
                        "evaluation": {
                            "dataset": {
                                "path": "bdsaglam/musique",
                                "name": "answerable",
                                "split": "validation"
                            }
                        },
                        "run": 1
                    }
                }
            },
            "metrics": {
                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                    "data": {
                        "exact_match": 0.586568361921958,
                        "f1": 0.6939285759126523,
                        "fuzzy_match": 0.6545290400240746,
                        "2hops": {
                            "exact_match": 0.6145659432387313,
                            "f1": 0.7205072396670257,
                            "fuzzy_match": 0.6814134668892599
                        },
                        "3hops": {
                            "exact_match": 0.49988602689765216,
                            "f1": 0.6124656855355362,
                            "fuzzy_match": 0.5767038978801003
                        },
                        "4hops": {
                            "exact_match": 0.5676595744680851,
                            "f1": 0.6728926865097078,
                            "fuzzy_match": 0.6161702127659574
                        }
                    }
                },
                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                    "data": {
                        "exact_match": 0.55,
                        "f1": 0.6629834609834611,
                        "fuzzy_match": 0.61,
                        "2hops": {
                            "exact_match": 0.6,
                            "f1": 0.7348075258075258,
                            "fuzzy_match": 0.68
                        },
                        "3hops": {
                            "exact_match": 0.59,
                            "f1": 0.6825476190476191,
                            "fuzzy_match": 0.64
                        },
                        "4hops": {
                            "exact_match": 0.46,
                            "f1": 0.5715952380952382,
                            "fuzzy_match": 0.51
                        }
                    }
                }
            },
            "deps": {
                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                    "hash": null,
                    "size": null,
                    "nfiles": null
                },
                "data/raw/research-mhqa-evaluation/system-prompts/no-role.txt": {
                    "hash": null,
                    "size": null,
                    "nfiles": null
                },
                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                    "hash": null,
                    "size": null,
                    "nfiles": null
                },
                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                    "hash": null,
                    "size": null,
                    "nfiles": null
                },
                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                    "hash": null,
                    "size": null,
                    "nfiles": null
                },
                "data/generated/research-mhqa-evaluation/qa-results": {
                    "hash": null,
                    "size": null,
                    "nfiles": null
                },
                "pipelines/research-mhqa-evaluation/report.py": {
                    "hash": null,
                    "size": null,
                    "nfiles": null
                },
                "data/generated/research-mhqa-evaluation/evals": {
                    "hash": null,
                    "size": null,
                    "nfiles": null
                },
                "pipelines/qa-prompt-optim/main.py": {
                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                    "size": 6925,
                    "nfiles": null
                },
                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                    "size": 4,
                    "nfiles": null
                },
                "data/generated/qa-prompt-optim/training/trained-program.json": {
                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                    "size": 528,
                    "nfiles": null
                }
            },
            "outs": {
                "data/raw": {
                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                    "size": 11799,
                    "nfiles": 36,
                    "use_cache": true,
                    "is_data_source": true
                },
                "data/generated/research-mhqa-evaluation/qa-results": {
                    "hash": null,
                    "size": null,
                    "nfiles": null,
                    "use_cache": true,
                    "is_data_source": false
                },
                "data/generated/research-mhqa-evaluation/evals": {
                    "hash": null,
                    "size": null,
                    "nfiles": null,
                    "use_cache": true,
                    "is_data_source": false
                },
                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                    "hash": null,
                    "size": null,
                    "nfiles": null,
                    "use_cache": true,
                    "is_data_source": false
                },
                "data/generated/qa-prompt-optim/training/trained-program.json": {
                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                    "size": 528,
                    "nfiles": null,
                    "use_cache": true,
                    "is_data_source": false
                },
                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                    "size": 741013,
                    "nfiles": null,
                    "use_cache": true,
                    "is_data_source": false
                }
            },
            "meta": {}
        },
        "error": null,
        "experiments": null
    },
    {
        "rev": "d6e184dbebe6e4188166ebb7fc393b5142ceaced",
        "name": "master",
        "data": {
            "rev": "d6e184dbebe6e4188166ebb7fc393b5142ceaced",
            "timestamp": "2024-10-30T10:47:54",
            "params": {
                "pipelines/qa-prompt-optim/params.yaml": {
                    "data": {
                        "train": {
                            "dataset": {
                                "path": "bdsaglam/musique-mini",
                                "name": "answerable",
                                "split": "train"
                            },
                            "optimizer": "noop"
                        },
                        "qa": {
                            "model": "llama-3-70b-tgi",
                            "temperature": 0.1,
                            "technique": "standard"
                        },
                        "evaluation": {
                            "dataset": {
                                "path": "bdsaglam/musique",
                                "name": "answerable",
                                "split": "validation"
                            }
                        },
                        "run": 1
                    }
                },
                "pipelines/research-mhqa-evaluation/params.yaml": {
                    "data": {
                        "dataset": {
                            "path": "bdsaglam/musique",
                            "name": "answerable",
                            "split": "validation"
                        },
                        "qa": {
                            "model": "llama-3-70b-tgi",
                            "temperature": 0.1,
                            "system_prompt": "no-role.txt",
                            "user_prompt_template": "cq.txt",
                            "few_shot_examples": "empty.json"
                        },
                        "run": 1
                    }
                }
            },
            "metrics": {
                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                    "data": {
                        "exact_match": 0.55,
                        "f1": 0.6629834609834611,
                        "fuzzy_match": 0.61,
                        "2hops": {
                            "exact_match": 0.6,
                            "f1": 0.7348075258075258,
                            "fuzzy_match": 0.68
                        },
                        "3hops": {
                            "exact_match": 0.59,
                            "f1": 0.6825476190476191,
                            "fuzzy_match": 0.64
                        },
                        "4hops": {
                            "exact_match": 0.46,
                            "f1": 0.5715952380952382,
                            "fuzzy_match": 0.51
                        }
                    }
                },
                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                    "error": {
                        "type": "FileNotFoundError",
                        "msg": "[Errno 2] No storage files available: 'data/generated/research-mhqa-evaluation/reports/scores.json'"
                    }
                }
            },
            "deps": {
                "pipelines/qa-prompt-optim/main.py": {
                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                    "size": 6925,
                    "nfiles": null
                },
                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                    "size": 4,
                    "nfiles": null
                },
                "data/generated/qa-prompt-optim/training/trained-program.json": {
                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                    "size": 528,
                    "nfiles": null
                },
                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                    "hash": null,
                    "size": null,
                    "nfiles": null
                },
                "data/raw/research-mhqa-evaluation/system-prompts/no-role.txt": {
                    "hash": null,
                    "size": null,
                    "nfiles": null
                },
                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                    "hash": null,
                    "size": null,
                    "nfiles": null
                },
                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                    "hash": null,
                    "size": null,
                    "nfiles": null
                },
                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                    "hash": null,
                    "size": null,
                    "nfiles": null
                },
                "data/generated/research-mhqa-evaluation/qa-results": {
                    "hash": null,
                    "size": null,
                    "nfiles": null
                },
                "pipelines/research-mhqa-evaluation/report.py": {
                    "hash": null,
                    "size": null,
                    "nfiles": null
                },
                "data/generated/research-mhqa-evaluation/evals": {
                    "hash": null,
                    "size": null,
                    "nfiles": null
                }
            },
            "outs": {
                "data/raw": {
                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                    "size": 11799,
                    "nfiles": 36,
                    "use_cache": true,
                    "is_data_source": true
                },
                "data/generated/qa-prompt-optim/training/trained-program.json": {
                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                    "size": 528,
                    "nfiles": null,
                    "use_cache": true,
                    "is_data_source": false
                },
                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                    "size": 741013,
                    "nfiles": null,
                    "use_cache": true,
                    "is_data_source": false
                },
                "data/generated/research-mhqa-evaluation/qa-results": {
                    "hash": null,
                    "size": null,
                    "nfiles": null,
                    "use_cache": true,
                    "is_data_source": false
                },
                "data/generated/research-mhqa-evaluation/evals": {
                    "hash": null,
                    "size": null,
                    "nfiles": null,
                    "use_cache": true,
                    "is_data_source": false
                },
                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                    "hash": null,
                    "size": null,
                    "nfiles": null,
                    "use_cache": true,
                    "is_data_source": false
                }
            },
            "meta": {}
        },
        "error": null,
        "experiments": [
            {
                "revs": [
                    {
                        "rev": "7e9bbe3d98cdec1c77407592c1b6b8cb858d341f",
                        "name": "gulfy-beth",
                        "data": {
                            "rev": "7e9bbe3d98cdec1c77407592c1b6b8cb858d341f",
                            "timestamp": "2024-10-31T19:08:00",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "cot.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5370293752585851,
                                        "f1": 0.6595111909549376,
                                        "fuzzy_match": 0.6143980140670252,
                                        "2hops": {
                                            "exact_match": 0.6006389776357828,
                                            "f1": 0.7231909927146775,
                                            "fuzzy_match": 0.6717252396166135
                                        },
                                        "3hops": {
                                            "exact_match": 0.49473684210526314,
                                            "f1": 0.6316824712954743,
                                            "fuzzy_match": 0.6026315789473684
                                        },
                                        "4hops": {
                                            "exact_match": 0.41975308641975306,
                                            "f1": 0.5148759196907345,
                                            "fuzzy_match": 0.45925925925925926
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/cot.txt": {
                                    "hash": "88021337dd159156750cbb8a8f3e2dc6",
                                    "size": 175,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "dde852cf4722694f1d96e677d7623118.dir",
                                    "size": 10061663,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "dc6487be504d1c0562de8b49de8db58c.dir",
                                    "size": 390868,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "dde852cf4722694f1d96e677d7623118.dir",
                                    "size": 10061663,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "dc6487be504d1c0562de8b49de8db58c.dir",
                                    "size": 390868,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "00135772aeb3626dfe27baf116cbb0cb",
                                    "size": 9873089,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/c0dc287e00c3ac21978d0fa89e5528996c5ef29c/c0dc287e00c3ac21978d0fa89e5528996c5ef29c.out",
                        "pid": 2411660,
                        "returncode": 0,
                        "task_id": "c0dc287e00c3ac21978d0fa89e5528996c5ef29c"
                    }
                },
                "name": "gulfy-beth"
            },
            {
                "revs": [
                    {
                        "rev": "621a34207ad82542fe1f50942d8d2a3c51b1676b",
                        "name": "ropey-lama",
                        "data": {
                            "rev": "621a34207ad82542fe1f50942d8d2a3c51b1676b",
                            "timestamp": "2024-10-31T19:02:14",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "cot.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.533305750930906,
                                        "f1": 0.6575636831179676,
                                        "fuzzy_match": 0.6061232933388498,
                                        "2hops": {
                                            "exact_match": 0.5902555910543131,
                                            "f1": 0.7161011143298307,
                                            "fuzzy_match": 0.6613418530351438
                                        },
                                        "3hops": {
                                            "exact_match": 0.4986842105263158,
                                            "f1": 0.6307297012985867,
                                            "fuzzy_match": 0.5960526315789474
                                        },
                                        "4hops": {
                                            "exact_match": 0.4222222222222222,
                                            "f1": 0.5269586517734666,
                                            "fuzzy_match": 0.454320987654321
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/cot.txt": {
                                    "hash": "88021337dd159156750cbb8a8f3e2dc6",
                                    "size": 175,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "442aff3c1798c23bfd1f3cfcdb569443.dir",
                                    "size": 10062376,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "713dd7dd8a72efe325d0e41c8c875f42.dir",
                                    "size": 391492,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "442aff3c1798c23bfd1f3cfcdb569443.dir",
                                    "size": 10062376,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "713dd7dd8a72efe325d0e41c8c875f42.dir",
                                    "size": 391492,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "252fac0a188fc556ffa19b927e628758",
                                    "size": 9874471,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/01b705f1ed6191f6462b238c25a78f57035949b5/01b705f1ed6191f6462b238c25a78f57035949b5.out",
                        "pid": 2401452,
                        "returncode": 0,
                        "task_id": "01b705f1ed6191f6462b238c25a78f57035949b5"
                    }
                },
                "name": "ropey-lama"
            },
            {
                "revs": [
                    {
                        "rev": "7d4a8047a88f0846d2e2e31fe243cbed7d5b79a7",
                        "name": "bluer-weep",
                        "data": {
                            "rev": "7d4a8047a88f0846d2e2e31fe243cbed7d5b79a7",
                            "timestamp": "2024-10-31T19:02:07",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "cot.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5353744311129499,
                                        "f1": 0.6556249844885169,
                                        "fuzzy_match": 0.6069507654116674,
                                        "2hops": {
                                            "exact_match": 0.5894568690095847,
                                            "f1": 0.7110203580051204,
                                            "fuzzy_match": 0.6565495207667732
                                        },
                                        "3hops": {
                                            "exact_match": 0.5052631578947369,
                                            "f1": 0.6387328791779256,
                                            "fuzzy_match": 0.6065789473684211
                                        },
                                        "4hops": {
                                            "exact_match": 0.4246913580246914,
                                            "f1": 0.516076817558299,
                                            "fuzzy_match": 0.454320987654321
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/cot.txt": {
                                    "hash": "88021337dd159156750cbb8a8f3e2dc6",
                                    "size": 175,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "ee4a6d1d2fa04a91906df5512b4295ee.dir",
                                    "size": 10058922,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "c1b1909247fb1c753da3956e58f1ca74.dir",
                                    "size": 391006,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "ee4a6d1d2fa04a91906df5512b4295ee.dir",
                                    "size": 10058922,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "c1b1909247fb1c753da3956e58f1ca74.dir",
                                    "size": 391006,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "6f8e1170dd669074aa1d7657f85779ff",
                                    "size": 9870513,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/67efa6a7b4ab184a612e74db5d1d43f37fed2d9b/67efa6a7b4ab184a612e74db5d1d43f37fed2d9b.out",
                        "pid": 2401783,
                        "returncode": 0,
                        "task_id": "67efa6a7b4ab184a612e74db5d1d43f37fed2d9b"
                    }
                },
                "name": "bluer-weep"
            },
            {
                "revs": [
                    {
                        "rev": "bb79efe0047e39115bf9686b5a8b39d89172e876",
                        "name": "score-areg",
                        "data": {
                            "rev": "bb79efe0047e39115bf9686b5a8b39d89172e876",
                            "timestamp": "2024-10-31T19:00:03",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.0,
                                        "f1": 9.194134142417137e-05,
                                        "fuzzy_match": 0.0,
                                        "2hops": {
                                            "exact_match": 0.0,
                                            "f1": 0.0,
                                            "fuzzy_match": 0.0
                                        },
                                        "3hops": {
                                            "exact_match": 0.0,
                                            "f1": 0.0,
                                            "fuzzy_match": 0.0
                                        },
                                        "4hops": {
                                            "exact_match": 0.0,
                                            "f1": 0.0005486968449931412,
                                            "fuzzy_match": 0.0
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal.txt": {
                                    "hash": "277fdcc43a53ddedd4d2f4d30c96331f",
                                    "size": 81,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "2c1cfb56a9ce13c8af194074d4842a6b.dir",
                                    "size": 9604567,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "34e825f11c0acbadd47c82f531c9911e.dir",
                                    "size": 346978,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "2c1cfb56a9ce13c8af194074d4842a6b.dir",
                                    "size": 9604567,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "34e825f11c0acbadd47c82f531c9911e.dir",
                                    "size": 346978,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "03a27a1ee21f625aca2032c54a3c5285",
                                    "size": 9377463,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/49138e4b68fe8667b1bad1b59102e35dce784ddf/49138e4b68fe8667b1bad1b59102e35dce784ddf.out",
                        "pid": 2427620,
                        "returncode": 0,
                        "task_id": "49138e4b68fe8667b1bad1b59102e35dce784ddf"
                    }
                },
                "name": "score-areg"
            },
            {
                "revs": [
                    {
                        "rev": "e711a759e031193453919254922ebb033bb9e79f",
                        "name": "dosed-hows",
                        "data": {
                            "rev": "e711a759e031193453919254922ebb033bb9e79f",
                            "timestamp": "2024-10-31T18:59:50",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.0008274720728175424,
                                        "f1": 0.0008274720728175424,
                                        "fuzzy_match": 0.0008274720728175424,
                                        "2hops": {
                                            "exact_match": 0.0007987220447284345,
                                            "f1": 0.0007987220447284345,
                                            "fuzzy_match": 0.0007987220447284345
                                        },
                                        "3hops": {
                                            "exact_match": 0.0,
                                            "f1": 0.0,
                                            "fuzzy_match": 0.0
                                        },
                                        "4hops": {
                                            "exact_match": 0.0024691358024691358,
                                            "f1": 0.0024691358024691358,
                                            "fuzzy_match": 0.0024691358024691358
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal.txt": {
                                    "hash": "277fdcc43a53ddedd4d2f4d30c96331f",
                                    "size": 81,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "f7e34d716c6597a36b8fd83246690dea.dir",
                                    "size": 9601177,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "5e568df43390c60c011e4864cdbc02ca.dir",
                                    "size": 346935,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "f7e34d716c6597a36b8fd83246690dea.dir",
                                    "size": 9601177,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "5e568df43390c60c011e4864cdbc02ca.dir",
                                    "size": 346935,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "5b36d8d1cb801fba6ec45bfd46764013",
                                    "size": 9374037,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/016f38ce8ecadda18aa7f0b7382c1aa7ddd7f171/016f38ce8ecadda18aa7f0b7382c1aa7ddd7f171.out",
                        "pid": 2428106,
                        "returncode": 0,
                        "task_id": "016f38ce8ecadda18aa7f0b7382c1aa7ddd7f171"
                    }
                },
                "name": "dosed-hows"
            },
            {
                "revs": [
                    {
                        "rev": "8df3727d685ab6d0bfe491c22f0439c2a7041ee9",
                        "name": "sappy-shes",
                        "data": {
                            "rev": "8df3727d685ab6d0bfe491c22f0439c2a7041ee9",
                            "timestamp": "2024-10-31T18:52:25",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "excellent-qa-cte.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "cte-2-shot.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.6479106330161357,
                                        "f1": 0.7497808330306749,
                                        "fuzzy_match": 0.6983864294580058,
                                        "2hops": {
                                            "exact_match": 0.6629392971246006,
                                            "f1": 0.7625598326243698,
                                            "fuzzy_match": 0.7172523961661342
                                        },
                                        "3hops": {
                                            "exact_match": 0.6723684210526316,
                                            "f1": 0.7846053065760322,
                                            "fuzzy_match": 0.7210526315789474
                                        },
                                        "4hops": {
                                            "exact_match": 0.5555555555555556,
                                            "f1": 0.644926740720113,
                                            "fuzzy_match": 0.5975308641975309
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/excellent-qa-cte.txt": {
                                    "hash": "0effab8496c59a1f390a4fd3d26aeaff",
                                    "size": 518,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/cte-2-shot.json": {
                                    "hash": "5ce5053d023f6717df1ab4adf4c31230",
                                    "size": 1752,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "8436f44faa42d2d1cc7e8d6c63cf6350.dir",
                                    "size": 9800190,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "cbd7578b1aea7ec33afa372434b2fd5c.dir",
                                    "size": 398496,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "8436f44faa42d2d1cc7e8d6c63cf6350.dir",
                                    "size": 9800190,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "cbd7578b1aea7ec33afa372434b2fd5c.dir",
                                    "size": 398496,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "7b7d18cbf982521b835f8f3ef591e95b",
                                    "size": 9619519,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/1d460e1346e081deeff7018f093ed2d729924a9e/1d460e1346e081deeff7018f093ed2d729924a9e.out",
                        "pid": 2413468,
                        "returncode": 0,
                        "task_id": "1d460e1346e081deeff7018f093ed2d729924a9e"
                    }
                },
                "name": "sappy-shes"
            },
            {
                "revs": [
                    {
                        "rev": "42ca212a2c5b8713df2195645a96db4d2ff56ff2",
                        "name": "hi-fi-trey",
                        "data": {
                            "rev": "42ca212a2c5b8713df2195645a96db4d2ff56ff2",
                            "timestamp": "2024-10-31T18:51:43",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "excellent-qa-cte.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "cte-2-shot.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.6483243690525444,
                                        "f1": 0.751309394625449,
                                        "fuzzy_match": 0.6992139015308233,
                                        "2hops": {
                                            "exact_match": 0.6661341853035144,
                                            "f1": 0.7671732866827685,
                                            "fuzzy_match": 0.7228434504792333
                                        },
                                        "3hops": {
                                            "exact_match": 0.6671052631578948,
                                            "f1": 0.7817049429536697,
                                            "fuzzy_match": 0.7157894736842105
                                        },
                                        "4hops": {
                                            "exact_match": 0.5580246913580247,
                                            "f1": 0.6452298647854203,
                                            "fuzzy_match": 0.5950617283950618
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/excellent-qa-cte.txt": {
                                    "hash": "0effab8496c59a1f390a4fd3d26aeaff",
                                    "size": 518,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/cte-2-shot.json": {
                                    "hash": "5ce5053d023f6717df1ab4adf4c31230",
                                    "size": 1752,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "c16ab62214e820d366e5dc2da4c3b628.dir",
                                    "size": 9793126,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "2fff47c5fee9a17eb0510d6331b802b0.dir",
                                    "size": 397053,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "c16ab62214e820d366e5dc2da4c3b628.dir",
                                    "size": 9793126,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "2fff47c5fee9a17eb0510d6331b802b0.dir",
                                    "size": 397053,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "2e98432d3ed550d5ea3db4b9986148e0",
                                    "size": 9610996,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/a4943802bc2591262cf4bdfde172a7c24063ec04/a4943802bc2591262cf4bdfde172a7c24063ec04.out",
                        "pid": 2413227,
                        "returncode": 0,
                        "task_id": "a4943802bc2591262cf4bdfde172a7c24063ec04"
                    }
                },
                "name": "hi-fi-trey"
            },
            {
                "revs": [
                    {
                        "rev": "7317ce79346ae2e827924b71f7f1f425a3c73531",
                        "name": "gules-bawl",
                        "data": {
                            "rev": "7317ce79346ae2e827924b71f7f1f425a3c73531",
                            "timestamp": "2024-10-31T18:51:19",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "excellent-qa-cte.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "cte-2-shot.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.6499793131981796,
                                        "f1": 0.7530371130948683,
                                        "fuzzy_match": 0.7025237898220935,
                                        "2hops": {
                                            "exact_match": 0.6693290734824281,
                                            "f1": 0.7693791425120149,
                                            "fuzzy_match": 0.7252396166134185
                                        },
                                        "3hops": {
                                            "exact_match": 0.6710526315789473,
                                            "f1": 0.7851091270528615,
                                            "fuzzy_match": 0.7210526315789474
                                        },
                                        "4hops": {
                                            "exact_match": 0.5506172839506173,
                                            "f1": 0.6423335292964922,
                                            "fuzzy_match": 0.5975308641975309
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/excellent-qa-cte.txt": {
                                    "hash": "0effab8496c59a1f390a4fd3d26aeaff",
                                    "size": 518,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/cte-2-shot.json": {
                                    "hash": "5ce5053d023f6717df1ab4adf4c31230",
                                    "size": 1752,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "7732f31e58a5ec25dc9bc7e63643f1e9.dir",
                                    "size": 9799366,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "545da2f5f61c9cad6074ff58b6ed9732.dir",
                                    "size": 398155,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "7732f31e58a5ec25dc9bc7e63643f1e9.dir",
                                    "size": 9799366,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "545da2f5f61c9cad6074ff58b6ed9732.dir",
                                    "size": 398155,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "48e26587e6e74a5c1c27c39c1b1e02ed",
                                    "size": 9618355,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/ed6bdecc9ef4703a655f1f0b0f4c1431feae45cc/ed6bdecc9ef4703a655f1f0b0f4c1431feae45cc.out",
                        "pid": 2412289,
                        "returncode": 0,
                        "task_id": "ed6bdecc9ef4703a655f1f0b0f4c1431feae45cc"
                    }
                },
                "name": "gules-bawl"
            },
            {
                "revs": [
                    {
                        "rev": "ef6b05fd7ce1e5ce6017f3d799d0f4dfa32d6a33",
                        "name": "lated-much",
                        "data": {
                            "rev": "ef6b05fd7ce1e5ce6017f3d799d0f4dfa32d6a33",
                            "timestamp": "2024-10-31T18:35:29",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "excellent-qa-few.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "standard-2-shot.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5990897807199007,
                                        "f1": 0.7092353502405303,
                                        "fuzzy_match": 0.6570128258171287,
                                        "2hops": {
                                            "exact_match": 0.6365814696485623,
                                            "f1": 0.7418915081037728,
                                            "fuzzy_match": 0.7092651757188498
                                        },
                                        "3hops": {
                                            "exact_match": 0.5815789473684211,
                                            "f1": 0.705296205772212,
                                            "fuzzy_match": 0.6302631578947369
                                        },
                                        "4hops": {
                                            "exact_match": 0.5160493827160494,
                                            "f1": 0.6156754493791531,
                                            "fuzzy_match": 0.5456790123456791
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/excellent-qa-few.txt": {
                                    "hash": "6e209b60faa4e581de1a8907e797f7aa",
                                    "size": 254,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/standard-2-shot.json": {
                                    "hash": "a103f39c617bfcf05538c84c77f6616d",
                                    "size": 1553,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "a58cbb37ea6e653cc0e903accb0bfa2d.dir",
                                    "size": 9418725,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "28bd7f132218a673f8b1bb43e880704c.dir",
                                    "size": 391307,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "a58cbb37ea6e653cc0e903accb0bfa2d.dir",
                                    "size": 9418725,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "28bd7f132218a673f8b1bb43e880704c.dir",
                                    "size": 391307,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "d543cf200f3594ac0e457d9c28636406",
                                    "size": 9229518,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/64fee0250ce41752515af1de33338626f4e6203d/64fee0250ce41752515af1de33338626f4e6203d.out",
                        "pid": 2436520,
                        "returncode": 0,
                        "task_id": "64fee0250ce41752515af1de33338626f4e6203d"
                    }
                },
                "name": "lated-much"
            },
            {
                "revs": [
                    {
                        "rev": "455398cf6464c622656d0685d0c98d8e559dc0cc",
                        "name": "melic-jots",
                        "data": {
                            "rev": "455398cf6464c622656d0685d0c98d8e559dc0cc",
                            "timestamp": "2024-10-31T18:35:29",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "excellent-qa-few.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "standard-2-shot.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.600330988829127,
                                        "f1": 0.7104464386051146,
                                        "fuzzy_match": 0.6586677699627638,
                                        "2hops": {
                                            "exact_match": 0.6381789137380192,
                                            "f1": 0.7434346091277045,
                                            "fuzzy_match": 0.7108626198083067
                                        },
                                        "3hops": {
                                            "exact_match": 0.5802631578947368,
                                            "f1": 0.7044346769501568,
                                            "fuzzy_match": 0.631578947368421
                                        },
                                        "4hops": {
                                            "exact_match": 0.5209876543209877,
                                            "f1": 0.6197495234532271,
                                            "fuzzy_match": 0.5481481481481482
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/excellent-qa-few.txt": {
                                    "hash": "6e209b60faa4e581de1a8907e797f7aa",
                                    "size": 254,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/standard-2-shot.json": {
                                    "hash": "a103f39c617bfcf05538c84c77f6616d",
                                    "size": 1553,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "f684147e74f1d136cbfc5c0c5a796b09.dir",
                                    "size": 9418465,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "aac8fcd2683796eaec12e28fa32f1fa4.dir",
                                    "size": 391274,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "f684147e74f1d136cbfc5c0c5a796b09.dir",
                                    "size": 9418465,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "aac8fcd2683796eaec12e28fa32f1fa4.dir",
                                    "size": 391274,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "940ddbbbb43faeee6b597b774574a74e",
                                    "size": 9229250,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/72591716ddffc7a6aa405a4e170b977ba67a4892/72591716ddffc7a6aa405a4e170b977ba67a4892.out",
                        "pid": 2436045,
                        "returncode": 0,
                        "task_id": "72591716ddffc7a6aa405a4e170b977ba67a4892"
                    }
                },
                "name": "melic-jots"
            },
            {
                "revs": [
                    {
                        "rev": "f277b7dac7d5ab38125843ce5db3ea2e4fd0e965",
                        "name": "meaty-abbs",
                        "data": {
                            "rev": "f277b7dac7d5ab38125843ce5db3ea2e4fd0e965",
                            "timestamp": "2024-10-31T18:29:39",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.0004137360364087712,
                                        "f1": 0.0005056773778329426,
                                        "fuzzy_match": 0.0004137360364087712,
                                        "2hops": {
                                            "exact_match": 0.0007987220447284345,
                                            "f1": 0.0007987220447284345,
                                            "fuzzy_match": 0.0007987220447284345
                                        },
                                        "3hops": {
                                            "exact_match": 0.0,
                                            "f1": 0.0,
                                            "fuzzy_match": 0.0
                                        },
                                        "4hops": {
                                            "exact_match": 0.0,
                                            "f1": 0.0005486968449931412,
                                            "fuzzy_match": 0.0
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal.txt": {
                                    "hash": "277fdcc43a53ddedd4d2f4d30c96331f",
                                    "size": 81,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "0a4b9afa712eb2ada4a4d150ba81f0d4.dir",
                                    "size": 9604266,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "061422fe232bc9d0f92a4a7f9e21dfcf.dir",
                                    "size": 346969,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "0a4b9afa712eb2ada4a4d150ba81f0d4.dir",
                                    "size": 9604266,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "061422fe232bc9d0f92a4a7f9e21dfcf.dir",
                                    "size": 346969,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "538919bf9ce4e26ec05771a24d9230cc",
                                    "size": 9377162,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/c5d7da88531c5ccab1a107cf20030129add56cb7/c5d7da88531c5ccab1a107cf20030129add56cb7.out",
                        "pid": 2413972,
                        "returncode": 0,
                        "task_id": "c5d7da88531c5ccab1a107cf20030129add56cb7"
                    }
                },
                "name": "meaty-abbs"
            },
            {
                "revs": [
                    {
                        "rev": "561d162e1af01b555bb069dc8b9560e5ca04bd61",
                        "name": "suave-vows",
                        "data": {
                            "rev": "561d162e1af01b555bb069dc8b9560e5ca04bd61",
                            "timestamp": "2024-10-31T18:12:50",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "excellent-qa-few.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "standard-2-shot.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5999172527927182,
                                        "f1": 0.7091263490040983,
                                        "fuzzy_match": 0.658254033926355,
                                        "2hops": {
                                            "exact_match": 0.6365814696485623,
                                            "f1": 0.7403527291688278,
                                            "fuzzy_match": 0.7084664536741214
                                        },
                                        "3hops": {
                                            "exact_match": 0.5842105263157895,
                                            "f1": 0.7076802909852445,
                                            "fuzzy_match": 0.6368421052631579
                                        },
                                        "4hops": {
                                            "exact_match": 0.5160493827160494,
                                            "f1": 0.6153080184561666,
                                            "fuzzy_match": 0.5432098765432098
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/excellent-qa-few.txt": {
                                    "hash": "6e209b60faa4e581de1a8907e797f7aa",
                                    "size": 254,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/standard-2-shot.json": {
                                    "hash": "a103f39c617bfcf05538c84c77f6616d",
                                    "size": 1553,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "e0a842c476bfa12d1e459587999b6ebe.dir",
                                    "size": 9418757,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "36e45d67db0b3b29e36d8f81d19dff47.dir",
                                    "size": 391287,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "e0a842c476bfa12d1e459587999b6ebe.dir",
                                    "size": 9418757,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "36e45d67db0b3b29e36d8f81d19dff47.dir",
                                    "size": 391287,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "ab34274d6713ccea4ce317b9c80d3c10",
                                    "size": 9229562,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/bd221ceb9d576edb341796725da6663346a8acff/bd221ceb9d576edb341796725da6663346a8acff.out",
                        "pid": 2428595,
                        "returncode": 0,
                        "task_id": "bd221ceb9d576edb341796725da6663346a8acff"
                    }
                },
                "name": "suave-vows"
            },
            {
                "revs": [
                    {
                        "rev": "044d3fd9285ed7de94ae9631eed056936a113887",
                        "name": "ohmic-tola",
                        "data": {
                            "rev": "044d3fd9285ed7de94ae9631eed056936a113887",
                            "timestamp": "2024-10-31T18:12:26",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "excellent-qa-cot.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "cot-2-shot.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.48655357881671496,
                                        "f1": 0.5686032232666031,
                                        "fuzzy_match": 0.5440628878775341,
                                        "2hops": {
                                            "exact_match": 0.5638977635782748,
                                            "f1": 0.6551932626232537,
                                            "fuzzy_match": 0.6206070287539937
                                        },
                                        "3hops": {
                                            "exact_match": 0.43157894736842106,
                                            "f1": 0.5087134197253147,
                                            "fuzzy_match": 0.5039473684210526
                                        },
                                        "4hops": {
                                            "exact_match": 0.3506172839506173,
                                            "f1": 0.41330821441932547,
                                            "fuzzy_match": 0.38271604938271603
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/excellent-qa-cot.txt": {
                                    "hash": "5658759cf229361109257f0710eae328",
                                    "size": 399,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/cot-2-shot.json": {
                                    "hash": "b536995986f0bff92e731a4ad4d36f81",
                                    "size": 2115,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "3cb019a71b0fd03701c2372a2a329615.dir",
                                    "size": 10167009,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "7dc5555e116b8fb7026137a9d1c232ba.dir",
                                    "size": 381025,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "3cb019a71b0fd03701c2372a2a329615.dir",
                                    "size": 10167009,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "7dc5555e116b8fb7026137a9d1c232ba.dir",
                                    "size": 381025,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "7e630787a9803d2b6a298b1682fbc67a",
                                    "size": 9970343,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/fe5d66d8c3d5198034e73d312193cad6335cb8a0/fe5d66d8c3d5198034e73d312193cad6335cb8a0.out",
                        "pid": 2320850,
                        "returncode": 0,
                        "task_id": "fe5d66d8c3d5198034e73d312193cad6335cb8a0"
                    }
                },
                "name": "ohmic-tola"
            },
            {
                "revs": [
                    {
                        "rev": "490f1ffd118825d1a7af72ca13c6bd739df480b5",
                        "name": "randy-sale",
                        "data": {
                            "rev": "490f1ffd118825d1a7af72ca13c6bd739df480b5",
                            "timestamp": "2024-10-31T18:11:42",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "excellent-qa-cot.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "cot-2-shot.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.48655357881671496,
                                        "f1": 0.5664142044821581,
                                        "fuzzy_match": 0.5440628878775341,
                                        "2hops": {
                                            "exact_match": 0.5702875399361023,
                                            "f1": 0.6559618530000499,
                                            "fuzzy_match": 0.6206070287539937
                                        },
                                        "3hops": {
                                            "exact_match": 0.41578947368421054,
                                            "f1": 0.4994850997724959,
                                            "fuzzy_match": 0.49605263157894736
                                        },
                                        "4hops": {
                                            "exact_match": 0.36049382716049383,
                                            "f1": 0.415185719630164,
                                            "fuzzy_match": 0.39753086419753086
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/excellent-qa-cot.txt": {
                                    "hash": "5658759cf229361109257f0710eae328",
                                    "size": 399,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/cot-2-shot.json": {
                                    "hash": "b536995986f0bff92e731a4ad4d36f81",
                                    "size": 2115,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "fc6041d90bf430b6ed844d24bde3170e.dir",
                                    "size": 10167368,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "8ba77597489de9bcbde0ef8e0f559a1a.dir",
                                    "size": 381326,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "fc6041d90bf430b6ed844d24bde3170e.dir",
                                    "size": 10167368,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "8ba77597489de9bcbde0ef8e0f559a1a.dir",
                                    "size": 381326,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "808fbdb37d0a194ba347bb96f30fa611",
                                    "size": 9971059,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/ba6e8d23bd794fbf09d5342d4267fd9ccd8be0d4/ba6e8d23bd794fbf09d5342d4267fd9ccd8be0d4.out",
                        "pid": 2321201,
                        "returncode": 0,
                        "task_id": "ba6e8d23bd794fbf09d5342d4267fd9ccd8be0d4"
                    }
                },
                "name": "randy-sale"
            },
            {
                "revs": [
                    {
                        "rev": "9838b9bd57a9293a73a92f0b7f66de982b00de08",
                        "name": "flash-zone",
                        "data": {
                            "rev": "9838b9bd57a9293a73a92f0b7f66de982b00de08",
                            "timestamp": "2024-10-31T18:11:32",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "excellent-qa-cot.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "cot-2-shot.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.49151841125362017,
                                        "f1": 0.5719734472599016,
                                        "fuzzy_match": 0.5482002482416218,
                                        "2hops": {
                                            "exact_match": 0.5726837060702875,
                                            "f1": 0.6629355802270653,
                                            "fuzzy_match": 0.6269968051118211
                                        },
                                        "3hops": {
                                            "exact_match": 0.41973684210526313,
                                            "f1": 0.49810991793678766,
                                            "fuzzy_match": 0.4934210526315789
                                        },
                                        "4hops": {
                                            "exact_match": 0.37530864197530867,
                                            "f1": 0.4293850319776245,
                                            "fuzzy_match": 0.4074074074074074
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/excellent-qa-cot.txt": {
                                    "hash": "5658759cf229361109257f0710eae328",
                                    "size": 399,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/cot-2-shot.json": {
                                    "hash": "b536995986f0bff92e731a4ad4d36f81",
                                    "size": 2115,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "4d7a54cb55b3a3d6dd0381ea1ec24d6f.dir",
                                    "size": 10166037,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "cfdbe30f50cb3dd95606c3390c6a1d1d.dir",
                                    "size": 381263,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "4d7a54cb55b3a3d6dd0381ea1ec24d6f.dir",
                                    "size": 10166037,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "cfdbe30f50cb3dd95606c3390c6a1d1d.dir",
                                    "size": 381263,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "73754dbe791edee6bb90ca30565196d2",
                                    "size": 9969613,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/07b3f00786412663151aeff0389e50cec223c65d/07b3f00786412663151aeff0389e50cec223c65d.out",
                        "pid": 2320859,
                        "returncode": 0,
                        "task_id": "07b3f00786412663151aeff0389e50cec223c65d"
                    }
                },
                "name": "flash-zone"
            },
            {
                "revs": [
                    {
                        "rev": "40f81c37facc24e7a19f88300aeec575ed8ad77e",
                        "name": "fined-rugs",
                        "data": {
                            "rev": "40f81c37facc24e7a19f88300aeec575ed8ad77e",
                            "timestamp": "2024-10-31T16:17:36",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal-output-format.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5345469590401324,
                                        "f1": 0.6584984961614269,
                                        "fuzzy_match": 0.5924700041373604,
                                        "2hops": {
                                            "exact_match": 0.5870607028753994,
                                            "f1": 0.7182148705907905,
                                            "fuzzy_match": 0.65814696485623
                                        },
                                        "3hops": {
                                            "exact_match": 0.5210526315789473,
                                            "f1": 0.6542229802569323,
                                            "fuzzy_match": 0.5776315789473684
                                        },
                                        "4hops": {
                                            "exact_match": 0.39753086419753086,
                                            "f1": 0.48191699320303855,
                                            "fuzzy_match": 0.41728395061728396
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal-output-format.txt": {
                                    "hash": "a52a365ad92c7cfe0e821aac8ce9112c",
                                    "size": 96,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "6a75bbec5ca7b59f54f99f7aa18ca06e.dir",
                                    "size": 9730557,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "0dac200fe32556c967bd4fbc8f20db39.dir",
                                    "size": 472548,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "6a75bbec5ca7b59f54f99f7aa18ca06e.dir",
                                    "size": 9730557,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "0dac200fe32556c967bd4fbc8f20db39.dir",
                                    "size": 472548,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "70224fda2eb5899ae93316cd9fb7f46d",
                                    "size": 9621894,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/3923127a3fdeabbb97ee9b118bddb457f76b6357/3923127a3fdeabbb97ee9b118bddb457f76b6357.out",
                        "pid": 2401142,
                        "returncode": 0,
                        "task_id": "3923127a3fdeabbb97ee9b118bddb457f76b6357"
                    }
                },
                "name": "fined-rugs"
            },
            {
                "revs": [
                    {
                        "rev": "4a4bb2d591eacb2e3d19b845d39c0b644edfa44d",
                        "name": "tarot-he'd",
                        "data": {
                            "rev": "4a4bb2d591eacb2e3d19b845d39c0b644edfa44d",
                            "timestamp": "2024-10-31T16:14:45",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal-output-format.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5362019031857674,
                                        "f1": 0.6598837587088626,
                                        "fuzzy_match": 0.5932974762101779,
                                        "2hops": {
                                            "exact_match": 0.5894568690095847,
                                            "f1": 0.7183881429700565,
                                            "fuzzy_match": 0.6573482428115016
                                        },
                                        "3hops": {
                                            "exact_match": 0.5223684210526316,
                                            "f1": 0.6573143247717842,
                                            "fuzzy_match": 0.5828947368421052
                                        },
                                        "4hops": {
                                            "exact_match": 0.39753086419753086,
                                            "f1": 0.483847414751245,
                                            "fuzzy_match": 0.4148148148148148
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal-output-format.txt": {
                                    "hash": "a52a365ad92c7cfe0e821aac8ce9112c",
                                    "size": 96,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "2bb272262e32febf419e3a3d5a7f0687.dir",
                                    "size": 9726397,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "63b596ad85110365d4b89ffaa99f1ef4.dir",
                                    "size": 471545,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "2bb272262e32febf419e3a3d5a7f0687.dir",
                                    "size": 9726397,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "63b596ad85110365d4b89ffaa99f1ef4.dir",
                                    "size": 471545,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "0dfe07eff97a7719ddd1bc31872a7da0",
                                    "size": 9616509,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/3017b075317bd954efc3cfcc2f3574ec31ac998e/3017b075317bd954efc3cfcc2f3574ec31ac998e.out",
                        "pid": 2399368,
                        "returncode": 0,
                        "task_id": "3017b075317bd954efc3cfcc2f3574ec31ac998e"
                    }
                },
                "name": "tarot-he'd"
            },
            {
                "revs": [
                    {
                        "rev": "b013df2d491cdbb6bce7bdd280ace4aa205f1bb3",
                        "name": "garni-mast",
                        "data": {
                            "rev": "b013df2d491cdbb6bce7bdd280ace4aa205f1bb3",
                            "timestamp": "2024-10-31T16:14:27",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal-output-format.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5366156392221763,
                                        "f1": 0.6606726151958211,
                                        "fuzzy_match": 0.5928837401737691,
                                        "2hops": {
                                            "exact_match": 0.5894568690095847,
                                            "f1": 0.7189303780359036,
                                            "fuzzy_match": 0.6573482428115016
                                        },
                                        "3hops": {
                                            "exact_match": 0.525,
                                            "f1": 0.6594406973738626,
                                            "fuzzy_match": 0.5815789473684211
                                        },
                                        "4hops": {
                                            "exact_match": 0.3950617283950617,
                                            "f1": 0.4828887595634882,
                                            "fuzzy_match": 0.4148148148148148
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal-output-format.txt": {
                                    "hash": "a52a365ad92c7cfe0e821aac8ce9112c",
                                    "size": 96,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "ed139e6b60f984f6b264cbcdfe7a5db3.dir",
                                    "size": 9724329,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "b0df4eaa178b671ca104b6de31b9d486.dir",
                                    "size": 471086,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "ed139e6b60f984f6b264cbcdfe7a5db3.dir",
                                    "size": 9724329,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "b0df4eaa178b671ca104b6de31b9d486.dir",
                                    "size": 471086,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "4d3187bd91cbb4d2a8a69b9acefd454b",
                                    "size": 9614046,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/7c13e9b6755ddb696c4b4634e52b0435dc142533/7c13e9b6755ddb696c4b4634e52b0435dc142533.out",
                        "pid": 2399871,
                        "returncode": 0,
                        "task_id": "7c13e9b6755ddb696c4b4634e52b0435dc142533"
                    }
                },
                "name": "garni-mast"
            },
            {
                "revs": [
                    {
                        "rev": "b6b83bdb571563cba3ddd5e976a1be71d631f69b",
                        "name": "nodal-kale",
                        "data": {
                            "rev": "b6b83bdb571563cba3ddd5e976a1be71d631f69b",
                            "timestamp": "2024-10-31T15:41:15",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "cte.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5854364915184113,
                                        "f1": 0.6991245339501974,
                                        "fuzzy_match": 0.6466694249069094,
                                        "2hops": {
                                            "exact_match": 0.6190095846645367,
                                            "f1": 0.7283000396716901,
                                            "fuzzy_match": 0.6797124600638977
                                        },
                                        "3hops": {
                                            "exact_match": 0.5868421052631579,
                                            "f1": 0.7174673619081533,
                                            "fuzzy_match": 0.6618421052631579
                                        },
                                        "4hops": {
                                            "exact_match": 0.47901234567901235,
                                            "f1": 0.5745114909591965,
                                            "fuzzy_match": 0.5160493827160494
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/cte.txt": {
                                    "hash": "0d0d2d8712ac90d790ba7a8def4be6d3",
                                    "size": 301,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "76a85cd4ad5f4e123698f3df494d6c0f.dir",
                                    "size": 9814842,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "8ddf181baf26762bed8ac2efb99c16b0.dir",
                                    "size": 398370,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "76a85cd4ad5f4e123698f3df494d6c0f.dir",
                                    "size": 9814842,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "8ddf181baf26762bed8ac2efb99c16b0.dir",
                                    "size": 398370,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "6c9f99ea4241f78d736d131b4c56de65",
                                    "size": 9633674,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/e03270011a01608986402ea6cea9fa6d6254f250/e03270011a01608986402ea6cea9fa6d6254f250.out",
                        "pid": 2321122,
                        "returncode": 0,
                        "task_id": "e03270011a01608986402ea6cea9fa6d6254f250"
                    }
                },
                "name": "nodal-kale"
            },
            {
                "revs": [
                    {
                        "rev": "79fa8100d3614582ac4f53aa962ef655c9c5abc4",
                        "name": "gamic-link",
                        "data": {
                            "rev": "79fa8100d3614582ac4f53aa962ef655c9c5abc4",
                            "timestamp": "2024-10-31T15:41:00",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "cte.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5887463798096815,
                                        "f1": 0.7028978429177855,
                                        "fuzzy_match": 0.6524617294166322,
                                        "2hops": {
                                            "exact_match": 0.6253993610223643,
                                            "f1": 0.7328529626893389,
                                            "fuzzy_match": 0.6884984025559105
                                        },
                                        "3hops": {
                                            "exact_match": 0.5868421052631579,
                                            "f1": 0.7178009535056352,
                                            "fuzzy_match": 0.6605263157894737
                                        },
                                        "4hops": {
                                            "exact_match": 0.47901234567901235,
                                            "f1": 0.5823295120517343,
                                            "fuzzy_match": 0.5259259259259259
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/cte.txt": {
                                    "hash": "0d0d2d8712ac90d790ba7a8def4be6d3",
                                    "size": 301,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "7fac4de3aa616f2ef87b8e3394afe56e.dir",
                                    "size": 9811248,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "b9155b8ac212ad829728d7d46f2aff05.dir",
                                    "size": 397662,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "7fac4de3aa616f2ef87b8e3394afe56e.dir",
                                    "size": 9811248,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "b9155b8ac212ad829728d7d46f2aff05.dir",
                                    "size": 397662,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "f4c48e0a71c585ca646ca6982623eaf4",
                                    "size": 9629421,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/cf5511e03da8b2c5b2ea2820d42d3d479473d2f2/cf5511e03da8b2c5b2ea2820d42d3d479473d2f2.out",
                        "pid": 2321116,
                        "returncode": 0,
                        "task_id": "cf5511e03da8b2c5b2ea2820d42d3d479473d2f2"
                    }
                },
                "name": "gamic-link"
            },
            {
                "revs": [
                    {
                        "rev": "c6c98c1013c4c08a6c6d9f39c7c1abbd42127cf6",
                        "name": "snuff-craw",
                        "data": {
                            "rev": "c6c98c1013c4c08a6c6d9f39c7c1abbd42127cf6",
                            "timestamp": "2024-10-31T15:40:38",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "cte.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5887463798096815,
                                        "f1": 0.702679229642587,
                                        "fuzzy_match": 0.6541166735622673,
                                        "2hops": {
                                            "exact_match": 0.6214057507987221,
                                            "f1": 0.7281003668940862,
                                            "fuzzy_match": 0.6853035143769968
                                        },
                                        "3hops": {
                                            "exact_match": 0.593421052631579,
                                            "f1": 0.724172196322729,
                                            "fuzzy_match": 0.675
                                        },
                                        "4hops": {
                                            "exact_match": 0.47901234567901235,
                                            "f1": 0.5837609123196615,
                                            "fuzzy_match": 0.5185185185185185
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/cte.txt": {
                                    "hash": "0d0d2d8712ac90d790ba7a8def4be6d3",
                                    "size": 301,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "d08c4062dd072bd07c4ef83fff8334f6.dir",
                                    "size": 9806797,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "af051e81f1b1f86b7cb37cb85b227a51.dir",
                                    "size": 396626,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "d08c4062dd072bd07c4ef83fff8334f6.dir",
                                    "size": 9806797,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "af051e81f1b1f86b7cb37cb85b227a51.dir",
                                    "size": 396626,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "f8e7e8640be1b05b427d1f239dedc870",
                                    "size": 9623954,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/fc13a04d21f74788d12a514f333f9ded649bba3e/fc13a04d21f74788d12a514f333f9ded649bba3e.out",
                        "pid": 2321468,
                        "returncode": 0,
                        "task_id": "fc13a04d21f74788d12a514f333f9ded649bba3e"
                    }
                },
                "name": "snuff-craw"
            },
            {
                "revs": [
                    {
                        "rev": "e84a7d1e61240a2282074d62339577905ae8f3f0",
                        "name": "pupal-yoke",
                        "data": {
                            "rev": "e84a7d1e61240a2282074d62339577905ae8f3f0",
                            "timestamp": "2024-10-31T14:40:10",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal.txt",
                                            "user_prompt_template": "cq-sep.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.0,
                                        "f1": 0.0,
                                        "fuzzy_match": 0.0,
                                        "2hops": {
                                            "exact_match": 0.0,
                                            "f1": 0.0,
                                            "fuzzy_match": 0.0
                                        },
                                        "3hops": {
                                            "exact_match": 0.0,
                                            "f1": 0.0,
                                            "fuzzy_match": 0.0
                                        },
                                        "4hops": {
                                            "exact_match": 0.0,
                                            "f1": 0.0,
                                            "fuzzy_match": 0.0
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal.txt": {
                                    "hash": "277fdcc43a53ddedd4d2f4d30c96331f",
                                    "size": 81,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq-sep.txt": {
                                    "hash": "2100d7906a5933fcb76f607a0408d1cc",
                                    "size": 42,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "eaca2c08e8ead9a3119bd792da24d6de.dir",
                                    "size": 9597588,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "dcc20ef8fea8fa5068b02da47b37b606.dir",
                                    "size": 346895,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "eaca2c08e8ead9a3119bd792da24d6de.dir",
                                    "size": 9597588,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "dcc20ef8fea8fa5068b02da47b37b606.dir",
                                    "size": 346895,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "ea98a5874424b198a6fa6418d11c54b6",
                                    "size": 9365641,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/065cac03e3433fea545749899ae886da7a627564/065cac03e3433fea545749899ae886da7a627564.out",
                        "pid": 2320847,
                        "returncode": 0,
                        "task_id": "065cac03e3433fea545749899ae886da7a627564"
                    }
                },
                "name": "pupal-yoke"
            },
            {
                "revs": [
                    {
                        "rev": "4b96eb6abb67e60e5ed6c8d482ef807af9d3e24d",
                        "name": "moody-crag",
                        "data": {
                            "rev": "4b96eb6abb67e60e5ed6c8d482ef807af9d3e24d",
                            "timestamp": "2024-10-31T14:39:48",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal.txt",
                                            "user_prompt_template": "cq-sep.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.0,
                                        "f1": 9.194134142417137e-05,
                                        "fuzzy_match": 0.0,
                                        "2hops": {
                                            "exact_match": 0.0,
                                            "f1": 0.0,
                                            "fuzzy_match": 0.0
                                        },
                                        "3hops": {
                                            "exact_match": 0.0,
                                            "f1": 0.0,
                                            "fuzzy_match": 0.0
                                        },
                                        "4hops": {
                                            "exact_match": 0.0,
                                            "f1": 0.0005486968449931412,
                                            "fuzzy_match": 0.0
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal.txt": {
                                    "hash": "277fdcc43a53ddedd4d2f4d30c96331f",
                                    "size": 81,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq-sep.txt": {
                                    "hash": "2100d7906a5933fcb76f607a0408d1cc",
                                    "size": 42,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "cf3be76c7d292520690af3cf092e6849.dir",
                                    "size": 9597354,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "fa737aa8dba69dddbc7e894e104f10bb.dir",
                                    "size": 346964,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "cf3be76c7d292520690af3cf092e6849.dir",
                                    "size": 9597354,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "fa737aa8dba69dddbc7e894e104f10bb.dir",
                                    "size": 346964,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "1f2edf9b222c922d21a828e32e920488",
                                    "size": 9370278,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/44b36b259ce6a3b18783248bce25b23991cc66ce/44b36b259ce6a3b18783248bce25b23991cc66ce.out",
                        "pid": 2320857,
                        "returncode": 0,
                        "task_id": "44b36b259ce6a3b18783248bce25b23991cc66ce"
                    }
                },
                "name": "moody-crag"
            },
            {
                "revs": [
                    {
                        "rev": "3d6a5e1fa8651d818da65f7c4199ebf3775e525c",
                        "name": "lucky-here",
                        "data": {
                            "rev": "3d6a5e1fa8651d818da65f7c4199ebf3775e525c",
                            "timestamp": "2024-10-31T14:39:42",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal.txt",
                                            "user_prompt_template": "cq-sep.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.0,
                                        "f1": 0.0,
                                        "fuzzy_match": 0.0,
                                        "2hops": {
                                            "exact_match": 0.0,
                                            "f1": 0.0,
                                            "fuzzy_match": 0.0
                                        },
                                        "3hops": {
                                            "exact_match": 0.0,
                                            "f1": 0.0,
                                            "fuzzy_match": 0.0
                                        },
                                        "4hops": {
                                            "exact_match": 0.0,
                                            "f1": 0.0,
                                            "fuzzy_match": 0.0
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal.txt": {
                                    "hash": "277fdcc43a53ddedd4d2f4d30c96331f",
                                    "size": 81,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq-sep.txt": {
                                    "hash": "2100d7906a5933fcb76f607a0408d1cc",
                                    "size": 42,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "76c26ab1b308f3f80d25fe222b4971fc.dir",
                                    "size": 9596441,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "8aa178c189fe319584946f5cf98a2b63.dir",
                                    "size": 346895,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "76c26ab1b308f3f80d25fe222b4971fc.dir",
                                    "size": 9596441,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "8aa178c189fe319584946f5cf98a2b63.dir",
                                    "size": 346895,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "f3e863c091ef0fb9a50840592591037a",
                                    "size": 9364474,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/1558edb9557178d5cbcc1478fb32b69f00de95c2/1558edb9557178d5cbcc1478fb32b69f00de95c2.out",
                        "pid": 2320844,
                        "returncode": 0,
                        "task_id": "1558edb9557178d5cbcc1478fb32b69f00de95c2"
                    }
                },
                "name": "lucky-here"
            },
            {
                "revs": [
                    {
                        "rev": "61815a49b0e3969c687b2ca1c49b00066b75ef00",
                        "name": "azoic-hips",
                        "data": {
                            "rev": "61815a49b0e3969c687b2ca1c49b00066b75ef00",
                            "timestamp": "2024-10-31T14:38:07",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "excellent-qa-few.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5676458419528341,
                                        "f1": 0.6883539933416901,
                                        "fuzzy_match": 0.6359122879602813,
                                        "2hops": {
                                            "exact_match": 0.615814696485623,
                                            "f1": 0.729778763839066,
                                            "fuzzy_match": 0.6900958466453674
                                        },
                                        "3hops": {
                                            "exact_match": 0.5473684210526316,
                                            "f1": 0.6940906861456396,
                                            "fuzzy_match": 0.6289473684210526
                                        },
                                        "4hops": {
                                            "exact_match": 0.4567901234567901,
                                            "f1": 0.5495300447152299,
                                            "fuzzy_match": 0.48148148148148145
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/excellent-qa-few.txt": {
                                    "hash": "6e209b60faa4e581de1a8907e797f7aa",
                                    "size": 254,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "7e6d327ba9d72a9240b2eb520e8bf0af.dir",
                                    "size": 9426181,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "9c39c755108dd77dbf209cdf5b23d751.dir",
                                    "size": 393689,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "7e6d327ba9d72a9240b2eb520e8bf0af.dir",
                                    "size": 9426181,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "9c39c755108dd77dbf209cdf5b23d751.dir",
                                    "size": 393689,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "e17be1b0cb9698e43986e9045b84ad98",
                                    "size": 9239054,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/51f1e5eeb572ce2af7e0c4d2aaf2d868996f39ef/51f1e5eeb572ce2af7e0c4d2aaf2d868996f39ef.out",
                        "pid": 2376895,
                        "returncode": 0,
                        "task_id": "51f1e5eeb572ce2af7e0c4d2aaf2d868996f39ef"
                    }
                },
                "name": "azoic-hips"
            },
            {
                "revs": [
                    {
                        "rev": "813a12cd6151d9b1895d089f7d5b679a5ae3b55e",
                        "name": "fluid-flap",
                        "data": {
                            "rev": "813a12cd6151d9b1895d089f7d5b679a5ae3b55e",
                            "timestamp": "2024-10-31T14:37:08",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "excellent-qa-few.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5688870500620604,
                                        "f1": 0.6888228184462226,
                                        "fuzzy_match": 0.6367397600330988,
                                        "2hops": {
                                            "exact_match": 0.615814696485623,
                                            "f1": 0.7291418213764131,
                                            "fuzzy_match": 0.6884984025559105
                                        },
                                        "3hops": {
                                            "exact_match": 0.55,
                                            "f1": 0.6945914785678718,
                                            "fuzzy_match": 0.6328947368421053
                                        },
                                        "4hops": {
                                            "exact_match": 0.45925925925925926,
                                            "f1": 0.5533572052090571,
                                            "fuzzy_match": 0.4839506172839506
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/excellent-qa-few.txt": {
                                    "hash": "6e209b60faa4e581de1a8907e797f7aa",
                                    "size": 254,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "5b4a6fc53e7cf6a6af9ac4282a418660.dir",
                                    "size": 9425329,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "dfe3cbfa7fc841ccd104391c54ef284b.dir",
                                    "size": 393481,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "5b4a6fc53e7cf6a6af9ac4282a418660.dir",
                                    "size": 9425329,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "dfe3cbfa7fc841ccd104391c54ef284b.dir",
                                    "size": 393481,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "2a6ed49681af2a0beec6c33b8fdea708",
                                    "size": 9237945,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/a06ae035e62134dbf79abf64b26900c884235823/a06ae035e62134dbf79abf64b26900c884235823.out",
                        "pid": 2376191,
                        "returncode": 0,
                        "task_id": "a06ae035e62134dbf79abf64b26900c884235823"
                    }
                },
                "name": "fluid-flap"
            },
            {
                "revs": [
                    {
                        "rev": "13e19d7b77d021ccee1c7e48714814a9004b4f14",
                        "name": "bijou-tirl",
                        "data": {
                            "rev": "13e19d7b77d021ccee1c7e48714814a9004b4f14",
                            "timestamp": "2024-10-31T13:59:40",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "excellent-qa-fewest.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "standard-2-shot.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.6015721969383533,
                                        "f1": 0.7003887007618637,
                                        "fuzzy_match": 0.6603227141083988,
                                        "2hops": {
                                            "exact_match": 0.6325878594249201,
                                            "f1": 0.7362723333608926,
                                            "fuzzy_match": 0.7028753993610224
                                        },
                                        "3hops": {
                                            "exact_match": 0.5973684210526315,
                                            "f1": 0.691935782655597,
                                            "fuzzy_match": 0.6473684210526316
                                        },
                                        "4hops": {
                                            "exact_match": 0.5135802469135803,
                                            "f1": 0.6053218112477372,
                                            "fuzzy_match": 0.5530864197530864
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/excellent-qa-fewest.txt": {
                                    "hash": "7fa9cb6698054df80d424f77d1ba3186",
                                    "size": 259,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/standard-2-shot.json": {
                                    "hash": "a103f39c617bfcf05538c84c77f6616d",
                                    "size": 1553,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "e2619dd692cf55281f4d35b0832e4ed9.dir",
                                    "size": 9421593,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "374440914c4a54b3ce5b69dbf9ae16a8.dir",
                                    "size": 391578,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "e2619dd692cf55281f4d35b0832e4ed9.dir",
                                    "size": 9421593,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "374440914c4a54b3ce5b69dbf9ae16a8.dir",
                                    "size": 391578,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "a72943e3496d3179abc7a64e2458edc1",
                                    "size": 9232882,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/212062b76b4123a5f1e55340f9758da544d9716e/212062b76b4123a5f1e55340f9758da544d9716e.out",
                        "pid": 2358386,
                        "returncode": 0,
                        "task_id": "212062b76b4123a5f1e55340f9758da544d9716e"
                    }
                },
                "name": "bijou-tirl"
            },
            {
                "revs": [
                    {
                        "rev": "8ed104e14ebcc0987fe10c73843c7a169b125045",
                        "name": "raked-door",
                        "data": {
                            "rev": "8ed104e14ebcc0987fe10c73843c7a169b125045",
                            "timestamp": "2024-10-31T13:59:30",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "excellent-qa-fewest.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "standard-2-shot.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.604054613156806,
                                        "f1": 0.7022451605042792,
                                        "fuzzy_match": 0.6611501861812163,
                                        "2hops": {
                                            "exact_match": 0.6381789137380192,
                                            "f1": 0.7398235732601541,
                                            "fuzzy_match": 0.7060702875399361
                                        },
                                        "3hops": {
                                            "exact_match": 0.5960526315789474,
                                            "f1": 0.6923456140522704,
                                            "fuzzy_match": 0.6447368421052632
                                        },
                                        "4hops": {
                                            "exact_match": 0.5135802469135803,
                                            "f1": 0.604653759351615,
                                            "fuzzy_match": 0.5530864197530864
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/excellent-qa-fewest.txt": {
                                    "hash": "7fa9cb6698054df80d424f77d1ba3186",
                                    "size": 259,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/standard-2-shot.json": {
                                    "hash": "a103f39c617bfcf05538c84c77f6616d",
                                    "size": 1553,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "a2a29f49e976c1e077ad5e870f408fd6.dir",
                                    "size": 9422661,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "db1cd75b9a53469c0b60c7d5ba13e481.dir",
                                    "size": 391925,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "a2a29f49e976c1e077ad5e870f408fd6.dir",
                                    "size": 9422661,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "db1cd75b9a53469c0b60c7d5ba13e481.dir",
                                    "size": 391925,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "ae75607386b073d46c8282f0d8742803",
                                    "size": 9234227,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/c4c5d1e89f243f195598ff66ad1e870fcbd416b0/c4c5d1e89f243f195598ff66ad1e870fcbd416b0.out",
                        "pid": 2358362,
                        "returncode": 0,
                        "task_id": "c4c5d1e89f243f195598ff66ad1e870fcbd416b0"
                    }
                },
                "name": "raked-door"
            },
            {
                "revs": [
                    {
                        "rev": "e95ba39b05de44be10dd70f0fbe1e2f59ecfb2f2",
                        "name": "crumb-lays",
                        "data": {
                            "rev": "e95ba39b05de44be10dd70f0fbe1e2f59ecfb2f2",
                            "timestamp": "2024-10-31T13:59:25",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "excellent-qa-fewest.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "standard-2-shot.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.6028134050475796,
                                        "f1": 0.7033904605576751,
                                        "fuzzy_match": 0.6611501861812163,
                                        "2hops": {
                                            "exact_match": 0.6349840255591054,
                                            "f1": 0.7388822792321467,
                                            "fuzzy_match": 0.7036741214057508
                                        },
                                        "3hops": {
                                            "exact_match": 0.5960526315789474,
                                            "f1": 0.6941425091492504,
                                            "fuzzy_match": 0.6473684210526316
                                        },
                                        "4hops": {
                                            "exact_match": 0.5160493827160494,
                                            "f1": 0.611026722508204,
                                            "fuzzy_match": 0.5555555555555556
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/excellent-qa-fewest.txt": {
                                    "hash": "7fa9cb6698054df80d424f77d1ba3186",
                                    "size": 259,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/standard-2-shot.json": {
                                    "hash": "a103f39c617bfcf05538c84c77f6616d",
                                    "size": 1553,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "b8fbb2d2d28f03d45634af279e9f3fc3.dir",
                                    "size": 9420785,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "c54309df422d3b664f3f606fa905282f.dir",
                                    "size": 391510,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "b8fbb2d2d28f03d45634af279e9f3fc3.dir",
                                    "size": 9420785,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "c54309df422d3b664f3f606fa905282f.dir",
                                    "size": 391510,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "abbc75a065861fa8881ad05963c45f7d",
                                    "size": 9231952,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/96f49f03a6a5c17b2c7526b45e12515fa1aa8686/96f49f03a6a5c17b2c7526b45e12515fa1aa8686.out",
                        "pid": 2358619,
                        "returncode": 0,
                        "task_id": "96f49f03a6a5c17b2c7526b45e12515fa1aa8686"
                    }
                },
                "name": "crumb-lays"
            },
            {
                "revs": [
                    {
                        "rev": "5b067cb476b44c2e0ef6a842c71ab3b39d1af0a8",
                        "name": "wacky-esne",
                        "data": {
                            "rev": "5b067cb476b44c2e0ef6a842c71ab3b39d1af0a8",
                            "timestamp": "2024-10-31T13:56:47",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "excellent-qa-few.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5697145221348779,
                                        "f1": 0.6894551643019151,
                                        "fuzzy_match": 0.6367397600330988,
                                        "2hops": {
                                            "exact_match": 0.615814696485623,
                                            "f1": 0.7298069904642046,
                                            "fuzzy_match": 0.6884984025559105
                                        },
                                        "3hops": {
                                            "exact_match": 0.55,
                                            "f1": 0.6947391823862412,
                                            "fuzzy_match": 0.631578947368421
                                        },
                                        "4hops": {
                                            "exact_match": 0.4641975308641975,
                                            "f1": 0.5547975344271641,
                                            "fuzzy_match": 0.48641975308641977
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/excellent-qa-few.txt": {
                                    "hash": "6e209b60faa4e581de1a8907e797f7aa",
                                    "size": 254,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "3a8805a8e38db2a19de6ebefaa4af2b3.dir",
                                    "size": 9425881,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "79199351dae68b3aefffb77bd2c73fea.dir",
                                    "size": 393570,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "3a8805a8e38db2a19de6ebefaa4af2b3.dir",
                                    "size": 9425881,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "79199351dae68b3aefffb77bd2c73fea.dir",
                                    "size": 393570,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "9a4cf9ff47b5681ee4f17120b4f003e9",
                                    "size": 9238678,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/d29aa0e0eb0b6dad8bb1c1b1b69eadef59770cdf/d29aa0e0eb0b6dad8bb1c1b1b69eadef59770cdf.out",
                        "pid": 2358755,
                        "returncode": 0,
                        "task_id": "d29aa0e0eb0b6dad8bb1c1b1b69eadef59770cdf"
                    }
                },
                "name": "wacky-esne"
            },
            {
                "revs": [
                    {
                        "rev": "883d602f46bd489401c90d59fae4673166746eb9",
                        "name": "natal-libs",
                        "data": {
                            "rev": "883d602f46bd489401c90d59fae4673166746eb9",
                            "timestamp": "2024-10-31T13:56:23",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal-output-format-answer-cue.txt",
                                            "user_prompt_template": "cq-sep.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5693007860984691,
                                        "f1": 0.6929507727611907,
                                        "fuzzy_match": 0.6404633843607779,
                                        "2hops": {
                                            "exact_match": 0.6222044728434505,
                                            "f1": 0.7350269531288262,
                                            "fuzzy_match": 0.6972843450479234
                                        },
                                        "3hops": {
                                            "exact_match": 0.5394736842105263,
                                            "f1": 0.6925503774691081,
                                            "fuzzy_match": 0.6328947368421053
                                        },
                                        "4hops": {
                                            "exact_match": 0.4617283950617284,
                                            "f1": 0.5636295939999644,
                                            "fuzzy_match": 0.47901234567901235
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal-output-format-answer-cue.txt": {
                                    "hash": "bb5bc978acf0f8a75d991daf1db0b497",
                                    "size": 111,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq-sep.txt": {
                                    "hash": "2100d7906a5933fcb76f607a0408d1cc",
                                    "size": 42,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "1b8a85f3f8311e796d3abc6aebdd0525.dir",
                                    "size": 9427669,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "6e4d98c299578073c653b990261faf39.dir",
                                    "size": 394085,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "1b8a85f3f8311e796d3abc6aebdd0525.dir",
                                    "size": 9427669,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "6e4d98c299578073c653b990261faf39.dir",
                                    "size": 394085,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "9f22165065a153c0e102dfb88038b8cd",
                                    "size": 9240941,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/e7309c3c9566a90291de4689fbf09a277b314a54/e7309c3c9566a90291de4689fbf09a277b314a54.out",
                        "pid": 2358140,
                        "returncode": 0,
                        "task_id": "e7309c3c9566a90291de4689fbf09a277b314a54"
                    }
                },
                "name": "natal-libs"
            },
            {
                "revs": [
                    {
                        "rev": "d5e4c4df70f609183ebe140cc08531eeeaa90a62",
                        "name": "pseud-buoy",
                        "data": {
                            "rev": "d5e4c4df70f609183ebe140cc08531eeeaa90a62",
                            "timestamp": "2024-10-31T12:41:18",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal-output-format.txt",
                                            "user_prompt_template": "cq-sep.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5399255275134465,
                                        "f1": 0.6634784352288294,
                                        "fuzzy_match": 0.5995035167563095,
                                        "2hops": {
                                            "exact_match": 0.5926517571884984,
                                            "f1": 0.7211920628899339,
                                            "fuzzy_match": 0.6645367412140575
                                        },
                                        "3hops": {
                                            "exact_match": 0.5171052631578947,
                                            "f1": 0.6561213122742903,
                                            "fuzzy_match": 0.5789473684210527
                                        },
                                        "4hops": {
                                            "exact_match": 0.41975308641975306,
                                            "f1": 0.4988709083491927,
                                            "fuzzy_match": 0.43703703703703706
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal-output-format.txt": {
                                    "hash": "a52a365ad92c7cfe0e821aac8ce9112c",
                                    "size": 96,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq-sep.txt": {
                                    "hash": "2100d7906a5933fcb76f607a0408d1cc",
                                    "size": 42,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "aef38fcc75f94be1e4b9558ae8784dfa.dir",
                                    "size": 9701638,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "021bc51a1d73b9c0c094f0ecff29cf7c.dir",
                                    "size": 464936,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "aef38fcc75f94be1e4b9558ae8784dfa.dir",
                                    "size": 9701638,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "021bc51a1d73b9c0c094f0ecff29cf7c.dir",
                                    "size": 464936,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "b1e1e2caab02708f1d3391aa2f8c45ce",
                                    "size": 9585354,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/bae60626b3af0c1f99099da2d010431d8a25574b/bae60626b3af0c1f99099da2d010431d8a25574b.out",
                        "pid": 2320845,
                        "returncode": 0,
                        "task_id": "bae60626b3af0c1f99099da2d010431d8a25574b"
                    }
                },
                "name": "pseud-buoy"
            },
            {
                "revs": [
                    {
                        "rev": "1a96e7786dc4584a049ad1bc8217b83d4d0762fb",
                        "name": "rummy-mobs",
                        "data": {
                            "rev": "1a96e7786dc4584a049ad1bc8217b83d4d0762fb",
                            "timestamp": "2024-10-31T12:40:26",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal-output-format.txt",
                                            "user_prompt_template": "cq-sep.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5399255275134465,
                                        "f1": 0.6638841961915194,
                                        "fuzzy_match": 0.5999172527927182,
                                        "2hops": {
                                            "exact_match": 0.5958466453674122,
                                            "f1": 0.7249605816250685,
                                            "fuzzy_match": 0.6669329073482428
                                        },
                                        "3hops": {
                                            "exact_match": 0.5171052631578947,
                                            "f1": 0.6532034999348826,
                                            "fuzzy_match": 0.5815789473684211
                                        },
                                        "4hops": {
                                            "exact_match": 0.40987654320987654,
                                            "f1": 0.4951180099995209,
                                            "fuzzy_match": 0.4271604938271605
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal-output-format.txt": {
                                    "hash": "a52a365ad92c7cfe0e821aac8ce9112c",
                                    "size": 96,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq-sep.txt": {
                                    "hash": "2100d7906a5933fcb76f607a0408d1cc",
                                    "size": 42,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "503efc18c74f7a60e12cfc5e7ce1844a.dir",
                                    "size": 9699541,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "fbac714436b335a91b2c459e4e451a6f.dir",
                                    "size": 464455,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "503efc18c74f7a60e12cfc5e7ce1844a.dir",
                                    "size": 9699541,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "fbac714436b335a91b2c459e4e451a6f.dir",
                                    "size": 464455,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "15a6ca39931f7be526114a53bb7808a8",
                                    "size": 9582727,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/2a3538d21ed3094d3c6e767fd2b00cfc3127bf51/2a3538d21ed3094d3c6e767fd2b00cfc3127bf51.out",
                        "pid": 2320846,
                        "returncode": 0,
                        "task_id": "2a3538d21ed3094d3c6e767fd2b00cfc3127bf51"
                    }
                },
                "name": "rummy-mobs"
            },
            {
                "revs": [
                    {
                        "rev": "ce3432bab86380d0c33d9653b0c2184103619a28",
                        "name": "ducky-flip",
                        "data": {
                            "rev": "ce3432bab86380d0c33d9653b0c2184103619a28",
                            "timestamp": "2024-10-31T11:55:13",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal-output-format-answer-cue.txt",
                                            "user_prompt_template": "cq-sep.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5697145221348779,
                                        "f1": 0.694208747980951,
                                        "fuzzy_match": 0.643773272652048,
                                        "2hops": {
                                            "exact_match": 0.6230031948881789,
                                            "f1": 0.7371074737521859,
                                            "fuzzy_match": 0.700479233226837
                                        },
                                        "3hops": {
                                            "exact_match": 0.5407894736842105,
                                            "f1": 0.6937408536595843,
                                            "fuzzy_match": 0.6381578947368421
                                        },
                                        "4hops": {
                                            "exact_match": 0.45925925925925926,
                                            "f1": 0.562471451730711,
                                            "fuzzy_match": 0.47901234567901235
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal-output-format-answer-cue.txt": {
                                    "hash": "bb5bc978acf0f8a75d991daf1db0b497",
                                    "size": 111,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq-sep.txt": {
                                    "hash": "2100d7906a5933fcb76f607a0408d1cc",
                                    "size": 42,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "a000cfa77999cdd3a88361ebd36b3926.dir",
                                    "size": 9427457,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "6ace4dd2240da278920954d3d04726d5.dir",
                                    "size": 393974,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "a000cfa77999cdd3a88361ebd36b3926.dir",
                                    "size": 9427457,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "6ace4dd2240da278920954d3d04726d5.dir",
                                    "size": 393974,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "5630cca76672824e68fda2b7dd898620",
                                    "size": 9240619,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/32109f9bcc4601ff33d9da24d9ab3a12d9e70f7a/32109f9bcc4601ff33d9da24d9ab3a12d9e70f7a.out",
                        "pid": 2321190,
                        "returncode": 0,
                        "task_id": "32109f9bcc4601ff33d9da24d9ab3a12d9e70f7a"
                    }
                },
                "name": "ducky-flip"
            },
            {
                "revs": [
                    {
                        "rev": "25ba554abc99b9b268a2224614f2abf09503efb4",
                        "name": "stung-dean",
                        "data": {
                            "rev": "25ba554abc99b9b268a2224614f2abf09503efb4",
                            "timestamp": "2024-10-31T11:55:13",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal-output-format-answer-cue-least.txt",
                                            "user_prompt_template": "cq-sep.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5668183698800165,
                                        "f1": 0.6784667506828832,
                                        "fuzzy_match": 0.6280513032685147,
                                        "2hops": {
                                            "exact_match": 0.6174121405750799,
                                            "f1": 0.7265278541336538,
                                            "fuzzy_match": 0.6789137380191693
                                        },
                                        "3hops": {
                                            "exact_match": 0.5657894736842105,
                                            "f1": 0.6787709529892191,
                                            "fuzzy_match": 0.6394736842105263
                                        },
                                        "4hops": {
                                            "exact_match": 0.4123456790123457,
                                            "f1": 0.5293218240824383,
                                            "fuzzy_match": 0.44938271604938274
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal-output-format-answer-cue-least.txt": {
                                    "hash": "25f7c2f0e8ac8e8b0fd9482db43def40",
                                    "size": 112,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq-sep.txt": {
                                    "hash": "2100d7906a5933fcb76f607a0408d1cc",
                                    "size": 42,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "c95a215f153e6361f1bde95ea36b2ccd.dir",
                                    "size": 9416597,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "5ca34e0f3e95258fe84988ee9604a7f9.dir",
                                    "size": 391229,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "c95a215f153e6361f1bde95ea36b2ccd.dir",
                                    "size": 9416597,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "5ca34e0f3e95258fe84988ee9604a7f9.dir",
                                    "size": 391229,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "9871884a16cf2da06627c52f94f3b063",
                                    "size": 9227161,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/b55477c4a3b885b1c8ff7022525e17e45544f587/b55477c4a3b885b1c8ff7022525e17e45544f587.out",
                        "pid": 2320860,
                        "returncode": 0,
                        "task_id": "b55477c4a3b885b1c8ff7022525e17e45544f587"
                    }
                },
                "name": "stung-dean"
            },
            {
                "revs": [
                    {
                        "rev": "e943420b0e65c8fb34b3331f8196acf07d1b0cf1",
                        "name": "tubby-razz",
                        "data": {
                            "rev": "e943420b0e65c8fb34b3331f8196acf07d1b0cf1",
                            "timestamp": "2024-10-31T11:55:09",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal-output-format-answer-cue.txt",
                                            "user_prompt_template": "cq-sep.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5705419942076955,
                                        "f1": 0.6934940351537314,
                                        "fuzzy_match": 0.6425320645428216,
                                        "2hops": {
                                            "exact_match": 0.6214057507987221,
                                            "f1": 0.7356953274887058,
                                            "fuzzy_match": 0.6972843450479234
                                        },
                                        "3hops": {
                                            "exact_match": 0.5434210526315789,
                                            "f1": 0.6925410711149876,
                                            "fuzzy_match": 0.6368421052631579
                                        },
                                        "4hops": {
                                            "exact_match": 0.4641975308641975,
                                            "f1": 0.5648230096378244,
                                            "fuzzy_match": 0.4839506172839506
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal-output-format-answer-cue.txt": {
                                    "hash": "bb5bc978acf0f8a75d991daf1db0b497",
                                    "size": 111,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq-sep.txt": {
                                    "hash": "2100d7906a5933fcb76f607a0408d1cc",
                                    "size": 42,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "e07bb9db44c57c92ca213ab220035ecb.dir",
                                    "size": 9426897,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "81cca0098a6158716ff7be467bbde752.dir",
                                    "size": 393805,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "e07bb9db44c57c92ca213ab220035ecb.dir",
                                    "size": 9426897,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "81cca0098a6158716ff7be467bbde752.dir",
                                    "size": 393805,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "3c33a392c36324ba1fd92a304111df2a",
                                    "size": 9239924,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/9627a41df3c5ecf802d2f3a19dcc01b7d5ba5412/9627a41df3c5ecf802d2f3a19dcc01b7d5ba5412.out",
                        "pid": 2321194,
                        "returncode": 0,
                        "task_id": "9627a41df3c5ecf802d2f3a19dcc01b7d5ba5412"
                    }
                },
                "name": "tubby-razz"
            },
            {
                "revs": [
                    {
                        "rev": "160fbef201132bc9e6cdef2b8cf37be6fa0d7f5d",
                        "name": "axile-muss",
                        "data": {
                            "rev": "160fbef201132bc9e6cdef2b8cf37be6fa0d7f5d",
                            "timestamp": "2024-10-31T11:55:09",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal-output-format-answer-cue-least.txt",
                                            "user_prompt_template": "cq-sep.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5680595779892429,
                                        "f1": 0.6787760061385704,
                                        "fuzzy_match": 0.630947455523376,
                                        "2hops": {
                                            "exact_match": 0.6206070287539937,
                                            "f1": 0.7273727651625506,
                                            "fuzzy_match": 0.6837060702875399
                                        },
                                        "3hops": {
                                            "exact_match": 0.5631578947368421,
                                            "f1": 0.6765077377616078,
                                            "fuzzy_match": 0.6394736842105263
                                        },
                                        "4hops": {
                                            "exact_match": 0.4148148148148148,
                                            "f1": 0.5328025287767638,
                                            "fuzzy_match": 0.45185185185185184
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal-output-format-answer-cue-least.txt": {
                                    "hash": "25f7c2f0e8ac8e8b0fd9482db43def40",
                                    "size": 112,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq-sep.txt": {
                                    "hash": "2100d7906a5933fcb76f607a0408d1cc",
                                    "size": 42,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "1e5feac2a3d75bc86569a1187ec2ddf9.dir",
                                    "size": 9415741,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "ab89f7668032b293d510f02f0f8a532e.dir",
                                    "size": 391019,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "1e5feac2a3d75bc86569a1187ec2ddf9.dir",
                                    "size": 9415741,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "ab89f7668032b293d510f02f0f8a532e.dir",
                                    "size": 391019,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "aa4533230e2309b41d155a22803511ce",
                                    "size": 9226085,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/146f5f11b8a2e1436b03ebb6bd0e55db2d18febb/146f5f11b8a2e1436b03ebb6bd0e55db2d18febb.out",
                        "pid": 2321608,
                        "returncode": 0,
                        "task_id": "146f5f11b8a2e1436b03ebb6bd0e55db2d18febb"
                    }
                },
                "name": "axile-muss"
            },
            {
                "revs": [
                    {
                        "rev": "39eb6c6cf5ad9d231eadec0d4aa7e26b94fe0e1a",
                        "name": "bosom-trug",
                        "data": {
                            "rev": "39eb6c6cf5ad9d231eadec0d4aa7e26b94fe0e1a",
                            "timestamp": "2024-10-31T11:55:06",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal-output-format-answer-cue-least.txt",
                                            "user_prompt_template": "cq-sep.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5684733140256516,
                                        "f1": 0.6800936324960235,
                                        "fuzzy_match": 0.630947455523376,
                                        "2hops": {
                                            "exact_match": 0.6182108626198083,
                                            "f1": 0.7262500946611942,
                                            "fuzzy_match": 0.6805111821086262
                                        },
                                        "3hops": {
                                            "exact_match": 0.5697368421052632,
                                            "f1": 0.6836128873706274,
                                            "fuzzy_match": 0.6460526315789473
                                        },
                                        "4hops": {
                                            "exact_match": 0.4123456790123457,
                                            "f1": 0.5308034489515971,
                                            "fuzzy_match": 0.44938271604938274
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal-output-format-answer-cue-least.txt": {
                                    "hash": "25f7c2f0e8ac8e8b0fd9482db43def40",
                                    "size": 112,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq-sep.txt": {
                                    "hash": "2100d7906a5933fcb76f607a0408d1cc",
                                    "size": 42,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "d4cf02fff355910d7619857d44ce2f9d.dir",
                                    "size": 9415093,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "18a07b7e0b9b339720d5fe5b1b83f8e6.dir",
                                    "size": 390885,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "d4cf02fff355910d7619857d44ce2f9d.dir",
                                    "size": 9415093,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "18a07b7e0b9b339720d5fe5b1b83f8e6.dir",
                                    "size": 390885,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "15cf7e55098a0bcd16f096da1ca652f7",
                                    "size": 9225289,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/de7a615caa45ec7721e055e52b10549ab3f989db/de7a615caa45ec7721e055e52b10549ab3f989db.out",
                        "pid": 2321120,
                        "returncode": 0,
                        "task_id": "de7a615caa45ec7721e055e52b10549ab3f989db"
                    }
                },
                "name": "bosom-trug"
            },
            {
                "revs": [
                    {
                        "rev": "ce933eab18c8921d3152923887140e9803284aad",
                        "name": "diazo-snip",
                        "data": {
                            "rev": "ce933eab18c8921d3152923887140e9803284aad",
                            "timestamp": "2024-10-30T15:47:02",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "cot.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "cot-2-shot.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5204799338022342,
                                        "f1": 0.6294411018522693,
                                        "fuzzy_match": 0.5908150599917252,
                                        "2hops": {
                                            "exact_match": 0.5958466453674122,
                                            "f1": 0.7059725640256772,
                                            "fuzzy_match": 0.6645367412140575
                                        },
                                        "3hops": {
                                            "exact_match": 0.46578947368421053,
                                            "f1": 0.5778331128795525,
                                            "fuzzy_match": 0.5513157894736842
                                        },
                                        "4hops": {
                                            "exact_match": 0.39012345679012345,
                                            "f1": 0.4896995734032771,
                                            "fuzzy_match": 0.43703703703703706
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/cot.txt": {
                                    "hash": "88021337dd159156750cbb8a8f3e2dc6",
                                    "size": 175,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/cot-2-shot.json": {
                                    "hash": "b536995986f0bff92e731a4ad4d36f81",
                                    "size": 2115,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "36aabccbf4a44f7c1072f67ffea9f2eb.dir",
                                    "size": 10139451,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "56cc28115b952702895c197676246fdf.dir",
                                    "size": 388259,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "36aabccbf4a44f7c1072f67ffea9f2eb.dir",
                                    "size": 10139451,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "56cc28115b952702895c197676246fdf.dir",
                                    "size": 388259,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "03f591310c9b40e5e263282231ca2276",
                                    "size": 9949271,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/3cddd10f99c0ece4d96db48f6a7a0a90e8775d62/3cddd10f99c0ece4d96db48f6a7a0a90e8775d62.out",
                        "pid": 2135650,
                        "returncode": 0,
                        "task_id": "3cddd10f99c0ece4d96db48f6a7a0a90e8775d62"
                    }
                },
                "name": "diazo-snip"
            },
            {
                "revs": [
                    {
                        "rev": "463ee7e30963372250123c455d5198233d536fd3",
                        "name": "elect-wino",
                        "data": {
                            "rev": "463ee7e30963372250123c455d5198233d536fd3",
                            "timestamp": "2024-10-30T15:46:43",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "cot.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "cot-2-shot.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5192387256930079,
                                        "f1": 0.6285752348293769,
                                        "fuzzy_match": 0.5883326437732727,
                                        "2hops": {
                                            "exact_match": 0.5902555910543131,
                                            "f1": 0.7017878836072636,
                                            "fuzzy_match": 0.6589456869009584
                                        },
                                        "3hops": {
                                            "exact_match": 0.4710526315789474,
                                            "f1": 0.5838780316127652,
                                            "fuzzy_match": 0.5552631578947368
                                        },
                                        "4hops": {
                                            "exact_match": 0.39012345679012345,
                                            "f1": 0.4861249587175513,
                                            "fuzzy_match": 0.43209876543209874
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/cot.txt": {
                                    "hash": "88021337dd159156750cbb8a8f3e2dc6",
                                    "size": 175,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/cot-2-shot.json": {
                                    "hash": "b536995986f0bff92e731a4ad4d36f81",
                                    "size": 2115,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "ecd77ce820507ec4e0d07793641dc857.dir",
                                    "size": 10136561,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "94b1524680b84570d38452c14d20f8f2.dir",
                                    "size": 388690,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "ecd77ce820507ec4e0d07793641dc857.dir",
                                    "size": 10136561,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "94b1524680b84570d38452c14d20f8f2.dir",
                                    "size": 388690,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "a4af2ba1aeba4264a96064453b592cac",
                                    "size": 9946677,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/6851aae2fcdb10c1a56beeaa6112603ce3b912e8/6851aae2fcdb10c1a56beeaa6112603ce3b912e8.out",
                        "pid": 2135637,
                        "returncode": 0,
                        "task_id": "6851aae2fcdb10c1a56beeaa6112603ce3b912e8"
                    }
                },
                "name": "elect-wino"
            },
            {
                "revs": [
                    {
                        "rev": "60cc2e6ff08d0bdb7367382c09aa2243e93b5848",
                        "name": "pricy-mina",
                        "data": {
                            "rev": "60cc2e6ff08d0bdb7367382c09aa2243e93b5848",
                            "timestamp": "2024-10-30T15:46:37",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "cot.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "cot-2-shot.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5200661977658254,
                                        "f1": 0.6274533639277052,
                                        "fuzzy_match": 0.5904013239553165,
                                        "2hops": {
                                            "exact_match": 0.5926517571884984,
                                            "f1": 0.6992505481543826,
                                            "fuzzy_match": 0.6597444089456869
                                        },
                                        "3hops": {
                                            "exact_match": 0.4644736842105263,
                                            "f1": 0.5801823376571827,
                                            "fuzzy_match": 0.5539473684210526
                                        },
                                        "4hops": {
                                            "exact_match": 0.4,
                                            "f1": 0.4942086856901672,
                                            "fuzzy_match": 0.4444444444444444
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/cot.txt": {
                                    "hash": "88021337dd159156750cbb8a8f3e2dc6",
                                    "size": 175,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/cot-2-shot.json": {
                                    "hash": "b536995986f0bff92e731a4ad4d36f81",
                                    "size": 2115,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "2bb0a2abc3b07cd7d27f3a527f82f63f.dir",
                                    "size": 10133525,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "35f5750b500192f08ce6f91918dbfbcc.dir",
                                    "size": 387655,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "2bb0a2abc3b07cd7d27f3a527f82f63f.dir",
                                    "size": 10133525,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "35f5750b500192f08ce6f91918dbfbcc.dir",
                                    "size": 387655,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "a6aa6b81fd2d321b1528c415758341c7",
                                    "size": 9942741,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/71da6b861cf559205127a2c6684d838a779cbc0b/71da6b861cf559205127a2c6684d838a779cbc0b.out",
                        "pid": 2135638,
                        "returncode": 0,
                        "task_id": "71da6b861cf559205127a2c6684d838a779cbc0b"
                    }
                },
                "name": "pricy-mina"
            },
            {
                "revs": [
                    {
                        "rev": "13f75d3bd561db96119cedce218877c89dbdc8bf",
                        "name": "volar-tiff",
                        "data": {
                            "rev": "13f75d3bd561db96119cedce218877c89dbdc8bf",
                            "timestamp": "2024-10-30T15:03:37",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "cte.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "cte-2-shot.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.6235002068680182,
                                        "f1": 0.7409918920689543,
                                        "fuzzy_match": 0.6818369880016549,
                                        "2hops": {
                                            "exact_match": 0.6597444089456869,
                                            "f1": 0.7669865653360233,
                                            "fuzzy_match": 0.7244408945686901
                                        },
                                        "3hops": {
                                            "exact_match": 0.6289473684210526,
                                            "f1": 0.7668211241373007,
                                            "fuzzy_match": 0.6868421052631579
                                        },
                                        "4hops": {
                                            "exact_match": 0.5012345679012346,
                                            "f1": 0.6121633802113904,
                                            "fuzzy_match": 0.5407407407407407
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/cte.txt": {
                                    "hash": "0d0d2d8712ac90d790ba7a8def4be6d3",
                                    "size": 301,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/cte-2-shot.json": {
                                    "hash": "5ce5053d023f6717df1ab4adf4c31230",
                                    "size": 1752,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "bf29f7d37234e68b1656390408f7405c.dir",
                                    "size": 9825355,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "e5eb13e28780e649e33a0af22def862d.dir",
                                    "size": 404050,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "bf29f7d37234e68b1656390408f7405c.dir",
                                    "size": 9825355,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "e5eb13e28780e649e33a0af22def862d.dir",
                                    "size": 404050,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "ea6fe6591a99ec8fc14c2223deea8ed0",
                                    "size": 9649831,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/83e08bb81af247acd6f6033975a0e7b676f36ef9/83e08bb81af247acd6f6033975a0e7b676f36ef9.out",
                        "pid": 2136540,
                        "returncode": 0,
                        "task_id": "83e08bb81af247acd6f6033975a0e7b676f36ef9"
                    }
                },
                "name": "volar-tiff"
            },
            {
                "revs": [
                    {
                        "rev": "775b04058608cf77cf87af5752d46b5f4fef8c1c",
                        "name": "broch-loge",
                        "data": {
                            "rev": "775b04058608cf77cf87af5752d46b5f4fef8c1c",
                            "timestamp": "2024-10-30T15:03:21",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "cte.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "cte-2-shot.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.6305337194869673,
                                        "f1": 0.7447350447446701,
                                        "fuzzy_match": 0.6859743483657427,
                                        "2hops": {
                                            "exact_match": 0.6589456869009584,
                                            "f1": 0.7668876047456401,
                                            "fuzzy_match": 0.7228434504792333
                                        },
                                        "3hops": {
                                            "exact_match": 0.6421052631578947,
                                            "f1": 0.7729962883168267,
                                            "fuzzy_match": 0.6934210526315789
                                        },
                                        "4hops": {
                                            "exact_match": 0.5209876543209877,
                                            "f1": 0.6232201058902179,
                                            "fuzzy_match": 0.5580246913580247
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/cte.txt": {
                                    "hash": "0d0d2d8712ac90d790ba7a8def4be6d3",
                                    "size": 301,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/cte-2-shot.json": {
                                    "hash": "5ce5053d023f6717df1ab4adf4c31230",
                                    "size": 1752,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "22b2d8789bc65cb39b1703e56ee36ac1.dir",
                                    "size": 9823133,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "90a595b1cc8779f582b13486029b9e45.dir",
                                    "size": 403448,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "22b2d8789bc65cb39b1703e56ee36ac1.dir",
                                    "size": 9823133,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "90a595b1cc8779f582b13486029b9e45.dir",
                                    "size": 403448,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "7fb0c750d30136bd3371202d29c887b9",
                                    "size": 9646808,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/6d37c5070acd7474f628f29de887e549ae585b3e/6d37c5070acd7474f628f29de887e549ae585b3e.out",
                        "pid": 2135719,
                        "returncode": 0,
                        "task_id": "6d37c5070acd7474f628f29de887e549ae585b3e"
                    }
                },
                "name": "broch-loge"
            },
            {
                "revs": [
                    {
                        "rev": "ac87423c19a8c8b83d710269d16fc9e6f5d6df95",
                        "name": "hazel-bunt",
                        "data": {
                            "rev": "ac87423c19a8c8b83d710269d16fc9e6f5d6df95",
                            "timestamp": "2024-10-30T15:03:18",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "cte.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "cte-2-shot.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.627637567232106,
                                        "f1": 0.7438329944348051,
                                        "fuzzy_match": 0.6847331402565163,
                                        "2hops": {
                                            "exact_match": 0.6621405750798722,
                                            "f1": 0.7691883713116613,
                                            "fuzzy_match": 0.7236421725239617
                                        },
                                        "3hops": {
                                            "exact_match": 0.631578947368421,
                                            "f1": 0.7664118281173768,
                                            "fuzzy_match": 0.6868421052631579
                                        },
                                        "4hops": {
                                            "exact_match": 0.5135802469135803,
                                            "f1": 0.6230802896234995,
                                            "fuzzy_match": 0.5604938271604938
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/cte.txt": {
                                    "hash": "0d0d2d8712ac90d790ba7a8def4be6d3",
                                    "size": 301,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/cte-2-shot.json": {
                                    "hash": "5ce5053d023f6717df1ab4adf4c31230",
                                    "size": 1752,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "4db7943f2effa1a54c2ef70f4ee5a55b.dir",
                                    "size": 9822626,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "c478563837a00512f8ab35128cc1ea6f.dir",
                                    "size": 403264,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "4db7943f2effa1a54c2ef70f4ee5a55b.dir",
                                    "size": 9822626,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "c478563837a00512f8ab35128cc1ea6f.dir",
                                    "size": 403264,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "56f2837dc786707245c088696c89e696",
                                    "size": 9646394,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/d16812a8453c51e25497e428e221b5c2b8b07b83/d16812a8453c51e25497e428e221b5c2b8b07b83.out",
                        "pid": 2136828,
                        "returncode": 0,
                        "task_id": "d16812a8453c51e25497e428e221b5c2b8b07b83"
                    }
                },
                "name": "hazel-bunt"
            },
            {
                "revs": [
                    {
                        "rev": "4df5a3d1c4950a49cf9a0196fea647bdbf4fb96f",
                        "name": "leggy-maya",
                        "data": {
                            "rev": "4df5a3d1c4950a49cf9a0196fea647bdbf4fb96f",
                            "timestamp": "2024-10-30T13:44:11",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal-output-format.txt",
                                            "user_prompt_template": "cq-sep.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5403392635498552,
                                        "f1": 0.6637985839194157,
                                        "fuzzy_match": 0.5986760446834919,
                                        "2hops": {
                                            "exact_match": 0.59185303514377,
                                            "f1": 0.7214989523933062,
                                            "fuzzy_match": 0.6621405750798722
                                        },
                                        "3hops": {
                                            "exact_match": 0.5210526315789473,
                                            "f1": 0.6560346904163183,
                                            "fuzzy_match": 0.5815789473684211
                                        },
                                        "4hops": {
                                            "exact_match": 0.41728395061728396,
                                            "f1": 0.4999953684454485,
                                            "fuzzy_match": 0.4345679012345679
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal-output-format.txt": {
                                    "hash": "a52a365ad92c7cfe0e821aac8ce9112c",
                                    "size": 96,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq-sep.txt": {
                                    "hash": "2100d7906a5933fcb76f607a0408d1cc",
                                    "size": 42,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "39e17c1065e95ceab254a81b23bef162.dir",
                                    "size": 9702477,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "89d09c11da63ef9a3994de73f294fe4e.dir",
                                    "size": 465059,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "39e17c1065e95ceab254a81b23bef162.dir",
                                    "size": 9702477,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "89d09c11da63ef9a3994de73f294fe4e.dir",
                                    "size": 465059,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "519541c8267928e805b5e88d3da03c34",
                                    "size": 9586362,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/95c50fa892db7b4f532a4e105115c715876df8a8/95c50fa892db7b4f532a4e105115c715876df8a8.out",
                        "pid": 2135723,
                        "returncode": 0,
                        "task_id": "95c50fa892db7b4f532a4e105115c715876df8a8"
                    }
                },
                "name": "leggy-maya"
            },
            {
                "revs": [
                    {
                        "rev": "dc2e765aeb1b84f0c5db6b4640a1ed79e79dd782",
                        "name": "rival-hems",
                        "data": {
                            "rev": "dc2e765aeb1b84f0c5db6b4640a1ed79e79dd782",
                            "timestamp": "2024-10-30T13:28:59",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "excellent-qa-fewest.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5639222176251552,
                                        "f1": 0.6773554900178509,
                                        "fuzzy_match": 0.6326023996690112,
                                        "2hops": {
                                            "exact_match": 0.6134185303514377,
                                            "f1": 0.7246840055243204,
                                            "fuzzy_match": 0.6813099041533547
                                        },
                                        "3hops": {
                                            "exact_match": 0.5460526315789473,
                                            "f1": 0.6716201378607323,
                                            "fuzzy_match": 0.6342105263157894
                                        },
                                        "4hops": {
                                            "exact_match": 0.4444444444444444,
                                            "f1": 0.5418087399568882,
                                            "fuzzy_match": 0.47901234567901235
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/excellent-qa-fewest.txt": {
                                    "hash": "7fa9cb6698054df80d424f77d1ba3186",
                                    "size": 259,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "23f4be2b17b0a1d92f9e0ec6c3295eaa.dir",
                                    "size": 9425057,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "6624f54ccfa27fced041d44743ed57f6.dir",
                                    "size": 393176,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "23f4be2b17b0a1d92f9e0ec6c3295eaa.dir",
                                    "size": 9425057,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "6624f54ccfa27fced041d44743ed57f6.dir",
                                    "size": 393176,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "8dbd2bed0c511e3d3c72c392c6579b37",
                                    "size": 9237749,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/528c1f684b7a2988400beb262c49eaeb156513a9/528c1f684b7a2988400beb262c49eaeb156513a9.out",
                        "pid": 2136539,
                        "returncode": 0,
                        "task_id": "528c1f684b7a2988400beb262c49eaeb156513a9"
                    }
                },
                "name": "rival-hems"
            },
            {
                "revs": [
                    {
                        "rev": "4f8f489211c96904b06236669e86c3e568dedb52",
                        "name": "white-hask",
                        "data": {
                            "rev": "4f8f489211c96904b06236669e86c3e568dedb52",
                            "timestamp": "2024-10-30T13:28:51",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "excellent-qa-fewest.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.564335953661564,
                                        "f1": 0.6777751631465772,
                                        "fuzzy_match": 0.6326023996690112,
                                        "2hops": {
                                            "exact_match": 0.6134185303514377,
                                            "f1": 0.7247143198611561,
                                            "fuzzy_match": 0.6797124600638977
                                        },
                                        "3hops": {
                                            "exact_match": 0.5473684210526316,
                                            "f1": 0.6725028231855451,
                                            "fuzzy_match": 0.6355263157894737
                                        },
                                        "4hops": {
                                            "exact_match": 0.4444444444444444,
                                            "f1": 0.5425631981187536,
                                            "fuzzy_match": 0.48148148148148145
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/excellent-qa-fewest.txt": {
                                    "hash": "7fa9cb6698054df80d424f77d1ba3186",
                                    "size": 259,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "8daca68cf983c66a6a80c5c243154e3f.dir",
                                    "size": 9424361,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "f5f62590ed66b3026fd1b946af4f4b49.dir",
                                    "size": 393003,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "8daca68cf983c66a6a80c5c243154e3f.dir",
                                    "size": 9424361,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "f5f62590ed66b3026fd1b946af4f4b49.dir",
                                    "size": 393003,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "1681be6f6693577aba5801a207dc67ff",
                                    "size": 9236860,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/8efb270fc054f5875aeec208151fe016a3effed9/8efb270fc054f5875aeec208151fe016a3effed9.out",
                        "pid": 2136336,
                        "returncode": 0,
                        "task_id": "8efb270fc054f5875aeec208151fe016a3effed9"
                    }
                },
                "name": "white-hask"
            },
            {
                "revs": [
                    {
                        "rev": "08c87a1db610fd53f41e0480d7b8ab7eba372dce",
                        "name": "briny-rims",
                        "data": {
                            "rev": "08c87a1db610fd53f41e0480d7b8ab7eba372dce",
                            "timestamp": "2024-10-30T13:28:51",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "excellent-qa-fewest.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5622672734795201,
                                        "f1": 0.6761195920096441,
                                        "fuzzy_match": 0.630947455523376,
                                        "2hops": {
                                            "exact_match": 0.6094249201277955,
                                            "f1": 0.7220781399246566,
                                            "fuzzy_match": 0.6773162939297125
                                        },
                                        "3hops": {
                                            "exact_match": 0.5486842105263158,
                                            "f1": 0.6732105478321492,
                                            "fuzzy_match": 0.6368421052631579
                                        },
                                        "4hops": {
                                            "exact_match": 0.4419753086419753,
                                            "f1": 0.539504213207917,
                                            "fuzzy_match": 0.4765432098765432
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/excellent-qa-fewest.txt": {
                                    "hash": "7fa9cb6698054df80d424f77d1ba3186",
                                    "size": 259,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "b67a2cb46ed4d929f9bd1eb0c4b3d12b.dir",
                                    "size": 9425809,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "0ff65d1e1198baffc23ba16d2cd9d99a.dir",
                                    "size": 393410,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "b67a2cb46ed4d929f9bd1eb0c4b3d12b.dir",
                                    "size": 9425809,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "0ff65d1e1198baffc23ba16d2cd9d99a.dir",
                                    "size": 393410,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "fa136699597f3fc03a5bed5999026fcd",
                                    "size": 9238728,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/c679f140270ab6b1cc2acae21209d4d8098bc28b/c679f140270ab6b1cc2acae21209d4d8098bc28b.out",
                        "pid": 2136808,
                        "returncode": 0,
                        "task_id": "c679f140270ab6b1cc2acae21209d4d8098bc28b"
                    }
                },
                "name": "briny-rims"
            },
            {
                "revs": [
                    {
                        "rev": "da42a9ed656b464663afea07c631760fd5a82559",
                        "name": "sunny-mash",
                        "data": {
                            "rev": "da42a9ed656b464663afea07c631760fd5a82559",
                            "timestamp": "2024-10-30T13:28:14",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal-output-format-answer-cue-least.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5676458419528341,
                                        "f1": 0.6798539258341856,
                                        "fuzzy_match": 0.6284650393049235,
                                        "2hops": {
                                            "exact_match": 0.6134185303514377,
                                            "f1": 0.722453068045198,
                                            "fuzzy_match": 0.6733226837060703
                                        },
                                        "3hops": {
                                            "exact_match": 0.5697368421052632,
                                            "f1": 0.6885782987795371,
                                            "fuzzy_match": 0.6473684210526316
                                        },
                                        "4hops": {
                                            "exact_match": 0.4222222222222222,
                                            "f1": 0.531793062904174,
                                            "fuzzy_match": 0.454320987654321
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal-output-format-answer-cue-least.txt": {
                                    "hash": "25f7c2f0e8ac8e8b0fd9482db43def40",
                                    "size": 112,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "e60ef8da0c8c0f288580afd73eb2abd4.dir",
                                    "size": 9420077,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "c41a0e8f6894be89ef52deee0bba1301.dir",
                                    "size": 392032,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "e60ef8da0c8c0f288580afd73eb2abd4.dir",
                                    "size": 9420077,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "c41a0e8f6894be89ef52deee0bba1301.dir",
                                    "size": 392032,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "714c52630bf17ffb102416476186b5bd",
                                    "size": 9231439,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/50149f6b20d20c16fa67167fc376dc215172a14e/50149f6b20d20c16fa67167fc376dc215172a14e.out",
                        "pid": 2136083,
                        "returncode": 0,
                        "task_id": "50149f6b20d20c16fa67167fc376dc215172a14e"
                    }
                },
                "name": "sunny-mash"
            },
            {
                "revs": [
                    {
                        "rev": "5064082b1a8bc73c707daec912627df644f6488c",
                        "name": "beery-dawk",
                        "data": {
                            "rev": "5064082b1a8bc73c707daec912627df644f6488c",
                            "timestamp": "2024-10-30T13:28:14",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal-output-format-answer-cue-least.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5676458419528341,
                                        "f1": 0.6792070642859239,
                                        "fuzzy_match": 0.6284650393049235,
                                        "2hops": {
                                            "exact_match": 0.6142172523961661,
                                            "f1": 0.7214640905596676,
                                            "fuzzy_match": 0.6749201277955271
                                        },
                                        "3hops": {
                                            "exact_match": 0.5723684210526315,
                                            "f1": 0.6890202021490721,
                                            "fuzzy_match": 0.6486842105263158
                                        },
                                        "4hops": {
                                            "exact_match": 0.4148148148148148,
                                            "f1": 0.5301606897903194,
                                            "fuzzy_match": 0.4469135802469136
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal-output-format-answer-cue-least.txt": {
                                    "hash": "25f7c2f0e8ac8e8b0fd9482db43def40",
                                    "size": 112,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "ad5ea9f2bed7b5528051816bdb8846b7.dir",
                                    "size": 9419085,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "124e2283666221a0cf9711d7b7ccfa4f.dir",
                                    "size": 391774,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "ad5ea9f2bed7b5528051816bdb8846b7.dir",
                                    "size": 9419085,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "124e2283666221a0cf9711d7b7ccfa4f.dir",
                                    "size": 391774,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "b8195143e3b05510214c36aeaa578175",
                                    "size": 9230214,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/d1d695b2cc0e4023ca400d799af81432b68cdc7c/d1d695b2cc0e4023ca400d799af81432b68cdc7c.out",
                        "pid": 2135636,
                        "returncode": 0,
                        "task_id": "d1d695b2cc0e4023ca400d799af81432b68cdc7c"
                    }
                },
                "name": "beery-dawk"
            },
            {
                "revs": [
                    {
                        "rev": "ce6df76fa5a3ec4e39e93338833dc01abef74679",
                        "name": "tinct-toby",
                        "data": {
                            "rev": "ce6df76fa5a3ec4e39e93338833dc01abef74679",
                            "timestamp": "2024-10-30T13:27:57",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal-output-format-answer-cue.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5734381464625569,
                                        "f1": 0.6991782811390698,
                                        "fuzzy_match": 0.6421183285064129,
                                        "2hops": {
                                            "exact_match": 0.6198083067092651,
                                            "f1": 0.7367964871012627,
                                            "fuzzy_match": 0.6940894568690096
                                        },
                                        "3hops": {
                                            "exact_match": 0.5513157894736842,
                                            "f1": 0.7057306539349884,
                                            "fuzzy_match": 0.6407894736842106
                                        },
                                        "4hops": {
                                            "exact_match": 0.47160493827160493,
                                            "f1": 0.5705911275845915,
                                            "fuzzy_match": 0.4839506172839506
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal-output-format-answer-cue.txt": {
                                    "hash": "bb5bc978acf0f8a75d991daf1db0b497",
                                    "size": 111,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "0798a1c129fdc7e09c51d944027a6fb4.dir",
                                    "size": 9429009,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "045c45d53f35819ed9a465a6de347e92.dir",
                                    "size": 394633,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "0798a1c129fdc7e09c51d944027a6fb4.dir",
                                    "size": 9429009,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "045c45d53f35819ed9a465a6de347e92.dir",
                                    "size": 394633,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "c083ab8562a6dd2076063b43a3b0ccd7",
                                    "size": 9242706,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/9bf6d0eb66e4ef8482310694d9540669b0478695/9bf6d0eb66e4ef8482310694d9540669b0478695.out",
                        "pid": 2136809,
                        "returncode": 0,
                        "task_id": "9bf6d0eb66e4ef8482310694d9540669b0478695"
                    }
                },
                "name": "tinct-toby"
            },
            {
                "revs": [
                    {
                        "rev": "cf646c7a8f1f50d284c0e92a6732c49a89c773af",
                        "name": "young-veil",
                        "data": {
                            "rev": "cf646c7a8f1f50d284c0e92a6732c49a89c773af",
                            "timestamp": "2024-10-30T13:27:40",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal-output-format-answer-cue-least.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5684733140256516,
                                        "f1": 0.6792536543968608,
                                        "fuzzy_match": 0.6301199834505585,
                                        "2hops": {
                                            "exact_match": 0.6110223642172524,
                                            "f1": 0.7197656871377062,
                                            "fuzzy_match": 0.6741214057507987
                                        },
                                        "3hops": {
                                            "exact_match": 0.575,
                                            "f1": 0.6906724074768315,
                                            "fuzzy_match": 0.65
                                        },
                                        "4hops": {
                                            "exact_match": 0.4246913580246914,
                                            "f1": 0.5325886733294141,
                                            "fuzzy_match": 0.4567901234567901
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal-output-format-answer-cue-least.txt": {
                                    "hash": "25f7c2f0e8ac8e8b0fd9482db43def40",
                                    "size": 112,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "3339495f03f1383e4dc4c1653a279a67.dir",
                                    "size": 9418545,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "11cdc34e71135aa5d7d802a7faf2b5d2.dir",
                                    "size": 391636,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "3339495f03f1383e4dc4c1653a279a67.dir",
                                    "size": 9418545,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "11cdc34e71135aa5d7d802a7faf2b5d2.dir",
                                    "size": 391636,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "338ce52e1838abc26739889e1d738963",
                                    "size": 9229535,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/bd5a68a3528c42a55761efefef451228739e59c1/bd5a68a3528c42a55761efefef451228739e59c1.out",
                        "pid": 2135714,
                        "returncode": 0,
                        "task_id": "bd5a68a3528c42a55761efefef451228739e59c1"
                    }
                },
                "name": "young-veil"
            },
            {
                "revs": [
                    {
                        "rev": "8d1d85a68fdc09577784a93f427a92cdd9117a18",
                        "name": "utile-genu",
                        "data": {
                            "rev": "8d1d85a68fdc09577784a93f427a92cdd9117a18",
                            "timestamp": "2024-10-30T13:27:37",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal-output-format-answer-cue.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5730244104261482,
                                        "f1": 0.6994027470222918,
                                        "fuzzy_match": 0.6417045924700041,
                                        "2hops": {
                                            "exact_match": 0.6190095846645367,
                                            "f1": 0.7362309709795837,
                                            "fuzzy_match": 0.6924920127795527
                                        },
                                        "3hops": {
                                            "exact_match": 0.5513157894736842,
                                            "f1": 0.7080000529226536,
                                            "fuzzy_match": 0.6407894736842106
                                        },
                                        "4hops": {
                                            "exact_match": 0.47160493827160493,
                                            "f1": 0.5694203053462312,
                                            "fuzzy_match": 0.48641975308641977
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal-output-format-answer-cue.txt": {
                                    "hash": "bb5bc978acf0f8a75d991daf1db0b497",
                                    "size": 111,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "4bad9ebf3c4d55adf16b7812a742d957.dir",
                                    "size": 9429357,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "2bb1be4da66ed1943b2e3465c3808970.dir",
                                    "size": 394719,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "4bad9ebf3c4d55adf16b7812a742d957.dir",
                                    "size": 9429357,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "2bb1be4da66ed1943b2e3465c3808970.dir",
                                    "size": 394719,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "08ccd3568f79c26a142a841b5b6c9811",
                                    "size": 9243115,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/8dc02717b1c4a8e996ebce4f6c0dbdb4e5a53038/8dc02717b1c4a8e996ebce4f6c0dbdb4e5a53038.out",
                        "pid": 2135657,
                        "returncode": 0,
                        "task_id": "8dc02717b1c4a8e996ebce4f6c0dbdb4e5a53038"
                    }
                },
                "name": "utile-genu"
            },
            {
                "revs": [
                    {
                        "rev": "92caac0879d374bfb72d2e39a9d54a07cf88a054",
                        "name": "bovid-mold",
                        "data": {
                            "rev": "92caac0879d374bfb72d2e39a9d54a07cf88a054",
                            "timestamp": "2024-10-30T13:27:26",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal-output-format-answer-cue.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5746793545717832,
                                        "f1": 0.7000524721360494,
                                        "fuzzy_match": 0.6441870086884568,
                                        "2hops": {
                                            "exact_match": 0.6246006389776357,
                                            "f1": 0.740071722492419,
                                            "fuzzy_match": 0.6972843450479234
                                        },
                                        "3hops": {
                                            "exact_match": 0.5486842105263158,
                                            "f1": 0.7062611468339022,
                                            "fuzzy_match": 0.6421052631578947
                                        },
                                        "4hops": {
                                            "exact_match": 0.4691358024691358,
                                            "f1": 0.5646877950581655,
                                            "fuzzy_match": 0.4839506172839506
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal-output-format-answer-cue.txt": {
                                    "hash": "bb5bc978acf0f8a75d991daf1db0b497",
                                    "size": 111,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "48a474831aeda3ad78e7d56d73fa6424.dir",
                                    "size": 9429257,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "7368fea9c3e7fe87a3217164b5b0e9a6.dir",
                                    "size": 394676,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "48a474831aeda3ad78e7d56d73fa6424.dir",
                                    "size": 9429257,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "7368fea9c3e7fe87a3217164b5b0e9a6.dir",
                                    "size": 394676,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "0000cecf70115573fb3fa9fa7fdadb12",
                                    "size": 9243001,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/e016f3d08fc6e337d7c4ccfd1dea65c53d350e72/e016f3d08fc6e337d7c4ccfd1dea65c53d350e72.out",
                        "pid": 2135717,
                        "returncode": 0,
                        "task_id": "e016f3d08fc6e337d7c4ccfd1dea65c53d350e72"
                    }
                },
                "name": "bovid-mold"
            },
            {
                "revs": [
                    {
                        "rev": "32184cf41adf5b6c7872aef0e3cdeb113ef707aa",
                        "name": "muddy-luke",
                        "data": {
                            "rev": "32184cf41adf5b6c7872aef0e3cdeb113ef707aa",
                            "timestamp": "2024-10-30T13:16:18",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal-output-format-answer-few-no-prio.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5664046338436077,
                                        "f1": 0.6906026615037606,
                                        "fuzzy_match": 0.6367397600330988,
                                        "2hops": {
                                            "exact_match": 0.6142172523961661,
                                            "f1": 0.7253019270899023,
                                            "fuzzy_match": 0.6884984025559105
                                        },
                                        "3hops": {
                                            "exact_match": 0.5368421052631579,
                                            "f1": 0.6959764276830841,
                                            "fuzzy_match": 0.6289473684210526
                                        },
                                        "4hops": {
                                            "exact_match": 0.4740740740740741,
                                            "f1": 0.5732507039478717,
                                            "fuzzy_match": 0.49135802469135803
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal-output-format-answer-few-no-prio.txt": {
                                    "hash": "a4ffbc2d4c7be6f3796396e6c32acb33",
                                    "size": 147,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "646310039cd17f4e428299379743ca94.dir",
                                    "size": 9428301,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "effafd0fd1242e800cd855f28eddba88.dir",
                                    "size": 394395,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "646310039cd17f4e428299379743ca94.dir",
                                    "size": 9428301,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "effafd0fd1242e800cd855f28eddba88.dir",
                                    "size": 394395,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "a12215f7a6208abc812c86707dc450c8",
                                    "size": 9241777,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/608b80a90ea0af3753c81084731b0e15c70ef613/608b80a90ea0af3753c81084731b0e15c70ef613.out",
                        "pid": 2132497,
                        "returncode": 0,
                        "task_id": "608b80a90ea0af3753c81084731b0e15c70ef613"
                    }
                },
                "name": "muddy-luke"
            },
            {
                "revs": [
                    {
                        "rev": "2029ad367cd0b1ee841676bfb418665c7391b48f",
                        "name": "oared-nark",
                        "data": {
                            "rev": "2029ad367cd0b1ee841676bfb418665c7391b48f",
                            "timestamp": "2024-10-30T13:16:17",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal-output-format-answer-few-no-prio.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5680595779892429,
                                        "f1": 0.6920395254279275,
                                        "fuzzy_match": 0.6379809681423252,
                                        "2hops": {
                                            "exact_match": 0.6166134185303515,
                                            "f1": 0.7273671389131853,
                                            "fuzzy_match": 0.6900958466453674
                                        },
                                        "3hops": {
                                            "exact_match": 0.5407894736842105,
                                            "f1": 0.6978456227295238,
                                            "fuzzy_match": 0.631578947368421
                                        },
                                        "4hops": {
                                            "exact_match": 0.4691358024691358,
                                            "f1": 0.5719338315198882,
                                            "fuzzy_match": 0.4888888888888889
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal-output-format-answer-few-no-prio.txt": {
                                    "hash": "a4ffbc2d4c7be6f3796396e6c32acb33",
                                    "size": 147,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "c826e5c06c43351671be6a41228460ec.dir",
                                    "size": 9427717,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "74deab83217b07a78e0abbc8842d9f36.dir",
                                    "size": 394237,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "c826e5c06c43351671be6a41228460ec.dir",
                                    "size": 9427717,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "74deab83217b07a78e0abbc8842d9f36.dir",
                                    "size": 394237,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "aee55857087296fd28fefb92a29fdd26",
                                    "size": 9241092,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/1b2733266bbd3ace202ea9ae89f76eefd4476010/1b2733266bbd3ace202ea9ae89f76eefd4476010.out",
                        "pid": 2132495,
                        "returncode": 0,
                        "task_id": "1b2733266bbd3ace202ea9ae89f76eefd4476010"
                    }
                },
                "name": "oared-nark"
            },
            {
                "revs": [
                    {
                        "rev": "8709ca59c9b429c22e7edd8c41bcb3d7a8ca3354",
                        "name": "bared-hill",
                        "data": {
                            "rev": "8709ca59c9b429c22e7edd8c41bcb3d7a8ca3354",
                            "timestamp": "2024-10-30T13:16:13",
                            "params": {
                                "pipelines/qa-prompt-optim/params.yaml": {
                                    "data": {
                                        "train": {
                                            "dataset": {
                                                "path": "bdsaglam/musique-mini",
                                                "name": "answerable",
                                                "split": "train"
                                            },
                                            "optimizer": "noop"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "technique": "standard"
                                        },
                                        "evaluation": {
                                            "dataset": {
                                                "path": "bdsaglam/musique",
                                                "name": "answerable",
                                                "split": "validation"
                                            }
                                        },
                                        "run": 1
                                    }
                                },
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique",
                                            "name": "answerable",
                                            "split": "validation"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "minimal-output-format-answer-few-no-prio.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/qa-prompt-optim/evaluation/scores.json": {
                                    "data": {
                                        "exact_match": 0.55,
                                        "f1": 0.6629834609834611,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.7348075258075258,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.59,
                                            "f1": 0.6825476190476191,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.5715952380952382,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                },
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5655771617707902,
                                        "f1": 0.6904509446338416,
                                        "fuzzy_match": 0.634671079851055,
                                        "2hops": {
                                            "exact_match": 0.615814696485623,
                                            "f1": 0.7271957333386599,
                                            "fuzzy_match": 0.6876996805111821
                                        },
                                        "3hops": {
                                            "exact_match": 0.5381578947368421,
                                            "f1": 0.6963460097264277,
                                            "fuzzy_match": 0.6289473684210526
                                        },
                                        "4hops": {
                                            "exact_match": 0.4617283950617284,
                                            "f1": 0.5657973028343398,
                                            "fuzzy_match": 0.48148148148148145
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/qa-prompt-optim/main.py": {
                                    "hash": "32a3b06f59271b13ab0119fc92c3b701",
                                    "size": 6925,
                                    "nfiles": null
                                },
                                "data/raw/qa-prompt-optim/optimizer-configs/noop.json": {
                                    "hash": "37a6259cc0c1dae299a7866489dff0bd",
                                    "size": 4,
                                    "nfiles": null
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/system-prompts/minimal-output-format-answer-few-no-prio.txt": {
                                    "hash": "a4ffbc2d4c7be6f3796396e6c32acb33",
                                    "size": 147,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/user-prompt-templates/cq.txt": {
                                    "hash": "ac4fa9e31f4b2f7d489ceaae22dbc7ea",
                                    "size": 41,
                                    "nfiles": null
                                },
                                "data/raw/research-mhqa-evaluation/few-shot-examples/empty.json": {
                                    "hash": "d751713988987e9331980363e24189ce",
                                    "size": 2,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "34d4315d723a34c6e1a6dd132f5d03af.dir",
                                    "size": 9428121,
                                    "nfiles": 2418
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "10bc3ac7c1ddb18ff9fdcce1afd44ed5.dir",
                                    "size": 394364,
                                    "nfiles": 2418
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "0598c65d8afd37282d02a8b1337ff922.dir",
                                    "size": 11799,
                                    "nfiles": 36,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/qa-prompt-optim/training/trained-program.json": {
                                    "hash": "e5542f827baec8c8a90f98af3f5cbf64",
                                    "size": 528,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/qa-prompt-optim/evaluation/results.jsonl": {
                                    "hash": "a5b8571b578ecf10fd095cd7ebcbe4dc",
                                    "size": 741013,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "34d4315d723a34c6e1a6dd132f5d03af.dir",
                                    "size": 9428121,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "10bc3ac7c1ddb18ff9fdcce1afd44ed5.dir",
                                    "size": 394364,
                                    "nfiles": 2418,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "a892fe117c56ed4a6a357ff87695171f",
                                    "size": 9241548,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/61a8bfbd35b80c3ac29cf6c0d82a62eb93db255e/61a8bfbd35b80c3ac29cf6c0d82a62eb93db255e.out",
                        "pid": 2132567,
                        "returncode": 0,
                        "task_id": "61a8bfbd35b80c3ac29cf6c0d82a62eb93db255e"
                    }
                },
                "name": "bared-hill"
            }
        ]
    }
]